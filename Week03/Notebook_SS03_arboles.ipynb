{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a952ad54-2218-4382-b2eb-d401d349f764",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"../banner/banner_ML_UNLP_1900_200.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523af81e",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/ML_UNLP_Lectures/blob/main/Week03/Notebook_SS03_arboles.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e57de4",
   "metadata": {},
   "source": [
    "# CARTs, Bagging, Random Forests y Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a3dcd",
   "metadata": {},
   "source": [
    "### Prediciendo precios de propiedades\n",
    "\n",
    "Nuestro objetivo hoy es construir un modelo para predecir los precios de la vivienda. Del artículo histórico de Rosen \"Hedonic Prices and Implicit Markets: Product Differentiation in Pure Competition\" (1974), sabemos que un vector de sus características describe un bien diferenciado.\n",
    "\n",
    "En el caso de una casa, estas características pueden incluir atributos estructurales (por ejemplo, número de dormitorios), amenidades, etc.. Así, podemos escribir el precio de mercado de la casa como:\n",
    "\n",
    "$$\n",
    "Price=f(atributos\\,estructurales,amenidades,...)\n",
    "$$\n",
    "\n",
    "\n",
    "Sin embargo, la teoría de Rosen no nos dice mucho sobre la forma funcional de $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9183a2e",
   "metadata": {},
   "source": [
    "# CARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab5a6",
   "metadata": {},
   "source": [
    "Let's load the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b3e2f",
   "metadata": {},
   "source": [
    " And the toy data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41492fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db= pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/toy_houses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=db, x='habitaciones', y='DCBD')\n",
    "\n",
    "# Scale x-axis and set labels\n",
    "plt.xticks(np.arange(0, 9, 1))\n",
    "plt.xlabel(\"Habitaciones\")\n",
    "plt.ylabel(\"Distancia al Centro\")\n",
    "\n",
    "# Apply classic theme\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Adjust text size and remove legend\n",
    "plt.rc('font', size=20)\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5668c",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "\n",
    "-  Datos: $y_{n\\times 1}$  y $X_{n\\times k}$ \n",
    "\n",
    "-  Definiciones\n",
    "\n",
    "      -  *j* es la variable que parte el espacio \n",
    "      - *s* es el punto de partición\n",
    "\n",
    "\n",
    "-  Definimos los siguientes semiplanos\n",
    "\n",
    "\\begin{align}\n",
    "R_1(j,s)=\\{X|X_j\\leq s\\} \\,\\,\\, \\& \\,\\,\\, R_2(j,s)=\\{X|X_j > s\\}\n",
    "\\end{align}\n",
    "\n",
    "-  *El problema*: buscar la variable de partición $X_j$ y el punto $s$ de forma tal que \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{j,s}{min} \\left[ \\underset{y_{R_1}}{min}\\sum_{x_i\\in R_1(j,s)}(y-y_{R_1})^2+ \\underset{y_{R_2}}{min}\\sum_{x_i\\in R_2(j,s)}(y-y_{R_2})^2\\right]\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872b7af",
   "metadata": {},
   "source": [
    "#### Algorithm by hand (\"artesanal\")\n",
    "\n",
    "1. Iniciemos por DBCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty list to store MSE values\n",
    "MSE_dbcd = []\n",
    "\n",
    "# Loop through the specified range\n",
    "for i in np.arange(1.25, 2, 0.25):\n",
    "    # Region 1\n",
    "    R1 = db[db['DCBD'] <= i]\n",
    "    yr1 = R1['price'].mean()\n",
    "    MSEr1 = ((R1['price'] - yr1) ** 2).mean() \n",
    "\n",
    "    # Region 2\n",
    "    R2 = db[db['DCBD'] > i]\n",
    "    yr2 = R2['price'].mean()\n",
    "    MSEr2 = ((R2['price'] - yr2) ** 2).mean()\n",
    "\n",
    "    # Store the sum of MSEs\n",
    "    MSE_dbcd.append(MSEr1 + MSEr2)\n",
    "\n",
    "# Output the result\n",
    "MSE_dbcd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1a57",
   "metadata": {},
   "source": [
    "2. Luego por Habitaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty list to store MSE values\n",
    "MSE_hab = []\n",
    "\n",
    "# Loop through the specified range\n",
    "for i in range(9):  # 0 to 8 inclusive\n",
    "    # Region 1\n",
    "    R1 = db[db['habitaciones'] <= i]\n",
    "    yr1 = R1['price'].mean()\n",
    "    MSEr1 = ((R1['price'] - yr1) ** 2).mean() \n",
    "\n",
    "    # Region 2\n",
    "    R2 = db[db['habitaciones'] > i]\n",
    "    yr2 = R2['price'].mean()\n",
    "    MSEr2 = ((R2['price'] - yr2) ** 2).mean() \n",
    "\n",
    "    # Store the sum of MSEs\n",
    "    MSE_hab.append(MSEr1 + MSEr2)\n",
    "\n",
    "# Output the result\n",
    "MSE_hab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd652993",
   "metadata": {},
   "source": [
    "**Mínimo?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f0a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the MSE lists\n",
    "MSE = MSE_dbcd + MSE_hab\n",
    "\n",
    "# Find the minimum MSE and its index\n",
    "min_MSE = min(MSE)\n",
    "min_MSE_index = MSE.index(min_MSE) + 1  # Adding 1 because Python indexing starts at 0\n",
    "\n",
    "# Output the minimum MSE and its index\n",
    "print(\"Minimum MSE is at index:\", min_MSE_index, \"with a value of:\", min_MSE)\n",
    "\n",
    "# Output the entire MSE list\n",
    "print(\"MSE values:\", MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29393e0",
   "metadata": {},
   "source": [
    "#### Algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8269f",
   "metadata": {},
   "source": [
    "La principal implementación de árboles de regresión en `Python` está disponible en la librería `scikit-learn` a través de las clase `DecisionTreeRegressor`. Una característica importante para aquellos que han utilizado otras implementaciones es que, en `scikit-learn`, es necesario convertir las variables categóricas en variables dummy (one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69588256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Prepare the features (X) and target (y)\n",
    "X = db[['DCBD', 'habitaciones']]\n",
    "y = db['price']\n",
    "\n",
    "# Create the decision tree model\n",
    "mytree = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "\n",
    "# Fit the model\n",
    "mytree.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a095abd",
   "metadata": {},
   "source": [
    "Una vez entrenado el árbol, se puede representar mediante la combinación de las funciones  `export_text()` y `plot_tree()`. \n",
    " - La función `export_text()` la estructura del árbol  y valor medio de la variable respuesta en cada nodo.  \n",
    " - La función `plot_tree()` dibuja la estructura del árbol y muestra el número de observaciones y valor medio de la variable respuesta en cada nodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "texto_modelo = export_text(\n",
    "                    decision_tree = mytree,\n",
    "                    feature_names = list(X.columns)\n",
    "               )\n",
    "print(texto_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "# Estructura del árbol creado\n",
    "# ------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "print(f\"Profundidad del árbol: {mytree.get_depth()}\")\n",
    "print(f\"Número de nodos terminales: {mytree.get_n_leaves()}\")\n",
    "\n",
    "plot = plot_tree(\n",
    "            decision_tree = mytree,\n",
    "            feature_names = X.columns,\n",
    "            class_names   = 'price',\n",
    "            filled        = True,\n",
    "            impurity      = False,\n",
    "            fontsize      = 10,\n",
    "            precision     = 2,\n",
    "            ax            = ax\n",
    "       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08f3fa",
   "metadata": {},
   "source": [
    "##### With California housing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing=pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/california_housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22abd8a5",
   "metadata": {},
   "source": [
    "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "\n",
    "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "\n",
    "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "\n",
    "4. totalRooms: Total number of rooms within a block\n",
    "\n",
    "5. totalBedrooms: Total number of bedrooms within a block\n",
    "\n",
    "6. population: Total number of people residing within a block\n",
    "\n",
    "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "\n",
    "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "\n",
    "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "\n",
    "10. oceanProximity: Location of the house w.r.t ocean/sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c48aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d98ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the missing data\n",
    "california_housing = california_housing.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data\n",
    "X = california_housing.drop('median_house_value', axis = 1)  # Features\n",
    "Y = california_housing['median_house_value']                 # Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree_california_1 = DecisionTreeRegressor(max_leaf_nodes = 8).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bbd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporing the categorical data\n",
    "california_housing['ocean_proximity'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical values to numeric values using one-hot encoding\n",
    "california_housing = pd.get_dummies(california_housing, columns= ['ocean_proximity'])\n",
    "\n",
    "# Showing the data after Converting categorical values to numeric values\n",
    "california_housing.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7074c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data\n",
    "X2 = california_housing.drop('median_house_value', axis = 1)  # Features\n",
    "Y = california_housing['median_house_value']                 # Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c555aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_california_2 = DecisionTreeRegressor(max_leaf_nodes = 4)\n",
    "\n",
    "tree_california_2.fit(X2,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura del árbol creado\n",
    "# ------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "print(f\"Profundidad del árbol: {tree_california_2.get_depth()}\")\n",
    "print(f\"Número de nodos terminales: {tree_california_2.get_n_leaves()}\")\n",
    "\n",
    "plot = plot_tree(\n",
    "            decision_tree = tree_california_2,\n",
    "            feature_names = X2.columns,\n",
    "            class_names   = 'median_house_value',\n",
    "            filled        = True,\n",
    "            impurity      = False,\n",
    "            fontsize      = 10,\n",
    "            precision     = 2,\n",
    "            ax            = ax\n",
    "       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10c96b",
   "metadata": {},
   "source": [
    "### Sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cce89d",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"figures/tree_uba.png\"  width=\"300\" height=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f410da",
   "metadata": {},
   "source": [
    "\n",
    "`DecisionTreeRegressor` del módulo `sklearn.tree` tiene  hiperparámetros, que nos permitirán \"controlar\" el sobreajuste:\n",
    "\n",
    "  - `max_depth`: profundidad máxima que puede alcanzar el árbol.\n",
    "\n",
    "  - `max_leaf_nodes`: número máximo de nodos terminales.\n",
    "\n",
    "  - `min_samples_leaf`: número mínimo de observaciones que debe de tener cada uno de los nodos hijos para que se produzca la división. Si es un valor decimal se interpreta como fracción del total de observaciones de entrenamiento `ceil(min_samples_split * n_samples)`.\n",
    "\n",
    "  \n",
    "  - `ccp_alpha`=0.0 Cost complexity prunning\n",
    "  \n",
    "  - `random_state`: semilla para que los resultados sean reproducibles. Tiene que ser un valor entero.\n",
    "\n",
    "Como en todo estudio de regresión, no solo es importante ajustar el modelo, sino también cuantificar su capacidad para predecir nuevas observaciones. Para poder hacer la posterior evaluación, se dividen los datos en dos grupos, uno de entrenamiento y otro de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de los datos en train y test\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X2,\n",
    "                                        Y,\n",
    "                                        test_size=0.3,\n",
    "                                        random_state = 1010101\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddbde6",
   "metadata": {},
   "source": [
    "#### Fijar la profundidad del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "param_grid = {'max_depth': [1, 5, 10, 15,20]}\n",
    "\n",
    "# Búsqueda por validación cruzada\n",
    "grid_depth = GridSearchCV(\n",
    "        # El árbol se crece al máximo posible para luego aplicar el pruning\n",
    "        estimator = DecisionTreeRegressor(random_state  = 123),\n",
    "        param_grid = param_grid,\n",
    "        cv         = 5,\n",
    "        refit      = True,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        return_train_score = True\n",
    "      )\n",
    "\n",
    "grid_depth.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb052b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor valor max_depth encontrado\n",
    "# ------------------------------------------------------------------------------\n",
    "grid_depth.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "scores = pd.DataFrame(grid_depth.cv_results_)\n",
    "scores.plot(x='param_max_depth', y='mean_train_score', ax=ax)\n",
    "scores.plot(x='param_max_depth', y='mean_test_score', ax=ax)\n",
    "ax.set_title(\"Error de validacion cruzada vs hiperparámetro max_depth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135403a9",
   "metadata": {},
   "source": [
    "#### Cost complexity Prunning\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "  C_{\\alpha}(T)= \\sum_{m=1}^{[T]}  \\sum_{x_i\\in R_m} (y_i-\\hat{y}_m)^2 + \\alpha [T]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "##### Algoritmo completo\n",
    "\n",
    "  - Hacemos crecer el árbol\n",
    "\n",
    "  - Para un dado $\\alpha$, aplicamos  *cost complexity pruning* \n",
    "    \n",
    "  - Utilizamos K-fold cross-validation para elegir $\\alpha$. \n",
    "\n",
    "  \n",
    "Tenemos entonces una secuencia de subarboles para distintos valores de $\\alpha$ \n",
    "\n",
    "Elegimos el $\\alpha$ y el subárbol que tienen el menor error de predicción.\n",
    "\n",
    "\n",
    "\n",
    "Para aplicar el proceso de pruning en `scikit-learn` es necesario indicar el argumento `ccp_alpha` que determina el grado de penalización por complejidad. Cuanto mayor es este valor, más agresivo el podado y menor el tamaño del árbol resultante. Dado que no hay forma de conocer de antemano el valor óptimo de `ccp_alpha`, se recurre a validación cruzada para identificarlo.\n",
    "\n",
    "Aunque existen otras formas de indentificar árboles \"optimos\", por ejemplo identificando el valor de `max_depth` y `min_samples_split` mediante validación cruzada, el pruning puede generar mejores resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning (const complexity pruning) por validación cruzada\n",
    "# ------------------------------------------------------------------------------\n",
    "# Valores de ccp_alpha evaluados\n",
    "param_grid = {'ccp_alpha':np.linspace(1000000, 10000000, 10)}\n",
    "\n",
    "# Búsqueda por validación cruzada\n",
    "grid = GridSearchCV(\n",
    "        # El árbol se crece al máximo posible para luego aplicar el pruning\n",
    "        estimator = DecisionTreeRegressor(\n",
    "                            max_depth         = None,\n",
    "                            min_samples_split = 2,\n",
    "                            min_samples_leaf  = 1,\n",
    "                            random_state      = 123\n",
    "                       ),\n",
    "        param_grid = param_grid,\n",
    "        cv         = 5,\n",
    "        refit      = True,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        return_train_score = True\n",
    "      )\n",
    "\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eacddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor valor ccp_alpha encontrado\n",
    "# ------------------------------------------------------------------------------\n",
    "grid.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ce332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "scores = pd.DataFrame(grid.cv_results_)\n",
    "scores.plot(x='param_ccp_alpha', y='mean_train_score', ax=ax)\n",
    "scores.plot(x='param_ccp_alpha', y='mean_test_score', ax=ax)\n",
    "ax.set_title(\"Error de validacion cruzada vs hiperparámetro ccp_alpha\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70ca1c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Una vez identificado el valor óptimo de `ccp_alpha`, se reentrena el árbol indicando este valor en sus argumentos. Si en el `GridSearchCV()` se indica `refit=True`, este reentrenamiento se hace automáticamente y el modelo resultante se encuentra almacenado en `.best_estimator_`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c54fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura del árbol final\n",
    "# ------------------------------------------------------------------------------\n",
    "modelo_prunning = grid.best_estimator_\n",
    "print(f\"Profundidad del árbol: {modelo_prunning.get_depth()}\")\n",
    "print(f\"Número de nodos terminales: {modelo_prunning.get_n_leaves()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382834c6",
   "metadata": {},
   "source": [
    "#### Predicción  y evaluación del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a7e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo max_depth\n",
    "#-------------------------------------------------------------------------------\n",
    "modelo_depth = grid_depth.best_estimator_\n",
    "predicciones = modelo_depth.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo final (tras aplicar pruning)\n",
    "#-------------------------------------------------------------------------------\n",
    "predicciones = modelo_prunning.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ba4df",
   "metadata": {},
   "source": [
    "### Comentarios sobre Árboles\n",
    "\n",
    "\n",
    "#### Pros: \n",
    "  \n",
    "    - Los árboles son muy fáciles de explicar a las personas (probablemente incluso más fáciles que la regresión lineal)\n",
    "\n",
    "    - Los árboles se pueden trazar gráficamente y son fácilmente interpretados incluso por no expertos. Variables más importantes en la parte superior\n",
    "\n",
    "\n",
    "\n",
    "#### Cons:\n",
    "    \n",
    "    - Si la estructura es lineal, CART no funciona bien\n",
    "    \n",
    "<div >\n",
    "<img src = \"figures/tree_vs_reg.png\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "    - Los árboles no son muy robustos \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1a513",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del modelo\n",
    "# ==============================================================================\n",
    "modelo_forest = RandomForestRegressor(\n",
    "            n_estimators = 10,\n",
    "            criterion    = 'squared_error',\n",
    "            max_depth    = None,\n",
    "            max_features = 1,\n",
    "            oob_score    = False,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 123\n",
    "         )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "# ==============================================================================\n",
    "modelo_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a55c9",
   "metadata": {},
   "source": [
    "### Predicción y evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e40992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo inicial\n",
    "# ==============================================================================\n",
    "predicciones_forest = modelo_forest.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones_forest,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b92de",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Optimización de hiperparámetros\n",
    "\n",
    "El modelo inicial se ha entrenado utilizando 10 árboles (n_estimators=10) y manteniendo el resto de hiperparámetros con su valor por defecto. Al ser hiperparámetros, no se puede saber de antemano cuál es el valor más adecuado, la forma de identificarlos es mediante el uso de estrategias de validación, por ejemplo validación cruzada.\n",
    "\n",
    "Los modelos Random Forest tienen la ventaja de disponer del Out-of-Bag error, lo que permite obtener una estimación del error de test sin recurrir a la validación cruzada, que es computacionalmente costosa. En la implementación de RandomForestRegressor, la métrica devuelta como oob_score es el 𝑅2, si se desea otra, se tiene que recurrir al método oob_decision_function_() para obtener las predicciones y con ellas calcular la métrica de interés. Para una explicación más detallada consultar: Grid search de modelos Random Forest con out-of-bag error y early stopping.\n",
    "\n",
    "Cabe tener en cuenta que, cuando se busca el valor óptimo de un hiperparámetro con dos métricas distintas, el resultado obtenido raramente es el mismo. Lo importante es que ambas métricas identifiquen las mismas regiones de interés.\n",
    "\n",
    "\n",
    "\n",
    "   La clase RandomForestRegressor del módulo sklearn.ensemble contiene varios hiperparámetros. De entre todos ellos, destacan aquellos que detienen el crecimiento de los árboles, los que controlan el número de árboles y predictores incluidos, y los que gestionan la paralelización:\n",
    "\n",
    "   - `n_estimators`; número de árboles incluidos en el modelo.\n",
    "\n",
    "   - `max_depth`: profundidad máxima que pueden alcanzar los árboles.\n",
    "\n",
    "   - `min_samples_split`: número mínimo de observaciones que debe de tener un nodo para que pueda dividirse. Si es un valor decimal se interpreta como fracción del total de observaciones de entrenamiento ceil(min_samples_split * n_samples).\n",
    "\n",
    "   - `min_samples_leaf`: número mínimo de observaciones que debe de tener cada uno de los nodos hijos para que se produzca la división. Si es un valor decimal se interpreta como fracción del total de observaciones de entrenamiento ceil(min_samples_split * n_samples).\n",
    "\n",
    "   - `max_leaf_nodes`: número máximo de nodos terminales que pueden tener los árboles.\n",
    "\n",
    "   - `max_features`: número de predictores considerados a en cada división. Puede ser:\n",
    "        Un valor entero\n",
    "        Una fracción del total de predictores..\n",
    "        “sqrt”, raiz cuadrada del número total de predictores.\n",
    "        “log2”, log2 del número total de predictores.\n",
    "        None, utiliza todos los predictores.\n",
    "\n",
    "   - `oob_score`: Si se calcula o no el out-of-bag R^2. Por defecto es False ya que aumenta el tiempo de entrenamiento.\n",
    "\n",
    "   - `n_jobs`: número de cores empleados para el entrenamiento. En random forest los árboles se ajustan de forma independiente, por lo la paralelización reduce notablemente el tiempo de entrenamiento. Con -1 se utilizan todos los cores disponibles.\n",
    "\n",
    "   - `random_state`: semilla para que los resultados sean reproducibles. Tiene que ser un valor entero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386aa52b",
   "metadata": {},
   "source": [
    "#### Número de árboles\n",
    "\n",
    "En Random Forest, el número de árboles no es un hiperparámetro crítico en cuanto que, añadir árboles, solo puede hacer que mejorar el resultado. En Random Forest no se produce overfitting por exceso de árboles. Sin embargo, añadir árboles una vez que la mejora se estabiliza es una perdida te recursos computacionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e876df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación empleando el Out-of-Bag error\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train_scores = []\n",
    "oob_scores   = []\n",
    "\n",
    "# Valores evaluados\n",
    "estimator_range = range(10, 150, 5)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de n_estimators y extraer su error\n",
    "# de entrenamiento y de Out-of-Bag.\n",
    "for n_estimators in estimator_range:\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = n_estimators,\n",
    "                criterion    = 'squared_error',\n",
    "                max_depth    = None,\n",
    "                max_features = 1,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 123\n",
    "             )\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(modelo.score(X_train, y_train))\n",
    "    oob_scores.append(modelo.oob_score_)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fa8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Gráfico con la evolución de los errores\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "ax.plot(estimator_range, oob_scores, label=\"out-of-bag scores\")\n",
    "ax.plot(estimator_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "        marker='o', color = \"red\", label=\"max score\")\n",
    "ax.set_ylabel(\"R^2\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_title(\"Evolución del out-of-bag-error vs número árboles\")\n",
    "plt.legend();\n",
    "print(f\"Valor óptimo de n_estimators: {estimator_range[np.argmax(oob_scores)]}\")\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123461b",
   "metadata": {},
   "source": [
    "#### Max features\n",
    "\n",
    "El valor de `max_features` es uno de los hiperparámetros más importantes de random forest, ya que es el que permite controlar cuánto se decorrelacionan los árboles entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing dataset correlation in heat map\n",
    "housing_dataset_correlation = california_housing.corr()\n",
    "plt.figure(figsize=(24,14))\n",
    "sns.heatmap(housing_dataset_correlation, annot = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación empleando el Out-of-Bag error\n",
    "# ==============================================================================\n",
    "train_scores = []\n",
    "oob_scores   = []\n",
    "\n",
    "# Valores evaluados\n",
    "max_features_range = range(1, X_train.shape[1] + 1, 1)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de max_features y extraer su error\n",
    "# de entrenamiento y de Out-of-Bag.\n",
    "for max_features in max_features_range:\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = 100,\n",
    "                criterion    = 'squared_error',\n",
    "                max_depth    = None,\n",
    "                max_features = max_features,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 123\n",
    "             )\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(modelo.score(X_train, y_train))\n",
    "    oob_scores.append(modelo.oob_score_)\n",
    "    \n",
    "# Gráfico con la evolución de los errores\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.plot(max_features_range, train_scores, label=\"train scores\")\n",
    "ax.plot(max_features_range, oob_scores, label=\"out-of-bag scores\")\n",
    "ax.plot(max_features_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "        marker='o', color = \"red\")\n",
    "ax.set_ylabel(\"R^2\")\n",
    "ax.set_xlabel(\"max_features\")\n",
    "ax.set_title(\"Evolución del out-of-bag-error vs número de predictores\")\n",
    "plt.legend();\n",
    "print(f\"Valor óptimo de max_features: {max_features_range[np.argmax(oob_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afc45a",
   "metadata": {},
   "source": [
    "##### Grid search\n",
    "\n",
    "Aunque el análisis individual de los hiperparámetros es útil para entender su impacto en el modelo e identificar rangos de interés, la búsqueda final no debe hacerse de forma secuencial, ya que cada hiperparámetro interacciona con los demás. Es preferible recurrir a grid search o random search para analizar varias combinaciones de hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f884326",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"figures/gridsearch.png\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe062f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Aleatorio basado en validación cruzada\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "\n",
    "# Grid de hiperparámetros evaluados\n",
    "# ==============================================================================\n",
    "param_grid = {'n_estimators': [100,150,200],\n",
    "              'max_features': [5, 7, 9],\n",
    "              'max_depth'   : [None, 3, 10, 20]\n",
    "             }\n",
    "\n",
    "# Búsqueda por grid search con validación cruzada\n",
    "# ==============================================================================\n",
    "grid_forest_cv = RandomizedSearchCV(\n",
    "            estimator  = RandomForestRegressor(random_state = 123),\n",
    "            param_distributions = param_grid,\n",
    "            scoring    = 'neg_root_mean_squared_error',\n",
    "            n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "            n_iter=2, \n",
    "            cv=5, \n",
    "            random_state=42,\n",
    "            refit=True\n",
    "       )\n",
    "\n",
    "grid_forest_cv.fit(X = X_train, y = y_train)\n",
    "\n",
    "# Resultados\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid_forest_cv.cv_results_)\n",
    "resultados.filter(regex = '(param.*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False) \\\n",
    "    .head(4)\n",
    "\n",
    "\n",
    "# Mejores hiperparámetros por validación cruzada\n",
    "# ==============================================================================\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Mejores hiperparámetros encontrados (cv)\")\n",
    "print(\"----------------------------------------\")\n",
    "print(grid_forest_cv.best_params_, \":\", grid_forest_cv.best_score_, grid_forest_cv.scoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1450d30",
   "metadata": {},
   "source": [
    "Una vez identificados los mejores hiperparámetros, se reentrena el modelo indicando los valores óptimos en sus argumentos. Si en el GridSearchCV() se indica refit=True, este reentrenamiento se hace automáticamente y el modelo resultante se encuentra almacenado en .best_estimator_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c085161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo final\n",
    "# ==============================================================================\n",
    "modelo_grid_forest_cv_final = grid_forest_cv.best_estimator_\n",
    "predicciones_grid_forest_cv_final = modelo_grid_forest_cv_final.predict(X = X_test)\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones_grid_forest_cv_final,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1732bc",
   "metadata": {},
   "source": [
    "# Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec803c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(\"https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/boosting_tree_toy.csv\")\n",
    "\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=db['y']\n",
    "x=db['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "\n",
    "# Adding labels and title for clarity\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=db['x'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a69ecb",
   "metadata": {},
   "source": [
    "### Hiperparámetros\n",
    "\n",
    "- $\\lambda$ la tasa a la que aprende, los valores típicos son 0.1, 0.01 o 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam =0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4219b8",
   "metadata": {},
   "source": [
    "- Tamaño del árbol. Arboles pocos profundos  funcionan bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66339a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=1 #stump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab704f53",
   "metadata": {},
   "source": [
    "- Iniciamos fijando $\\hat{f}(x)=0$ y $r_i=y_i$ para todos los $i$ del training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhat = np.zeros(len(y))\n",
    "\n",
    "r=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cafcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f8fd9",
   "metadata": {},
   "source": [
    "Para $m=1,2,...,M$\n",
    "\n",
    " - Ajustamos un árbol $\\hat{f}^m$ con $d$ bifurcaciones ($d+1$ hojas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primera iteración\n",
    "model = DecisionTreeRegressor(max_depth=d, random_state=12)\n",
    "model.fit(x, y)\n",
    "\n",
    "  # Make predictions\n",
    "yhat1 = model.predict(x)\n",
    "\n",
    "\n",
    "lam *yhat1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55ac8f",
   "metadata": {},
   "source": [
    "   - Actualizamos $\\hat{f}(x)$ con una versión \"shrunken\" del nuevo árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f776039",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=fhat + lam *yhat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6584eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of x vs y\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Add a step line plot of x vs f1\n",
    "plt.step(x, f1, where='mid', color='red', linewidth=3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38170e6",
   "metadata": {},
   "source": [
    "- Actualizamos los residuales\n",
    "  \\begin{align}\n",
    "  r_i\\leftarrow r_i-\\lambda\\hat{f}^m(x)\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92889de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "r1 = r - lam*yhat1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208fefa",
   "metadata": {},
   "source": [
    "El loop vuelve a iniciar, en la iteración 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteracion 2\n",
    "\n",
    "model.fit(x, r1)\n",
    "\n",
    "  # Make predictions\n",
    "yhat2 = model.predict(x)\n",
    "\n",
    "\n",
    "\n",
    "f2 = f1 + lam *yhat2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a79928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.step(x, f2, where='mid', color='red', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09734c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En un loop\n",
    "# Initialize fhat and residuals (r)\n",
    "fhat = np.zeros(len(y))\n",
    "r = y.copy()\n",
    "\n",
    "# Set lambda (adjust as needed)\n",
    "lambda_ = 0.1\n",
    "\n",
    "# Initialize YP with lambda * fhat\n",
    "YP = lambda_ * fhat.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "    # Fit a decision tree regressor\n",
    "    fit = DecisionTreeRegressor(max_depth=1)\n",
    "    fit.fit(x.reshape(-1, 1), r)\n",
    "\n",
    "    # Predict using the fitted model\n",
    "    yhat = fit.predict(x.reshape(-1, 1))\n",
    "\n",
    "    # Update residuals\n",
    "    r = r - lambda_ * yhat\n",
    "\n",
    "    # Update YP\n",
    "    YP = np.hstack((YP, (lambda_ * yhat).reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100\n",
    "fhat = np.apply_along_axis(np.sum, 1, YP[:, :M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb76c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of x vs y\n",
    "plt.scatter(x, y, label='Data')\n",
    "\n",
    "# Step plot of x vs fhat\n",
    "plt.step(x, fhat, where='mid', color='red', linewidth=3, label='Boosted Model')\n",
    "\n",
    "# Setting labels and showing the plot\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa7ab6",
   "metadata": {},
   "source": [
    "## Con los datos de California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6929793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Creación del modelo\n",
    "# ==============================================================================\n",
    "modelo_boost = HistGradientBoostingRegressor(\n",
    "            max_iter     = 600,\n",
    "            loss         = 'squared_error',\n",
    "            random_state = 123\n",
    "         )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "# ==============================================================================\n",
    "modelo_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Error de test del modelo inicial\n",
    "# ==============================================================================\n",
    "predicciones_boost = modelo_boost.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones_boost,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a564c0a",
   "metadata": {},
   "source": [
    "\n",
    "#### Ajuste del modelo, Hiperparámetros y Grid Search basado en validación cruzada\n",
    "\n",
    "Puede encontrarse una descripción detallada de todos ellos en sklearn.ensemble.GradientBoostingRegressor. En la práctica, cabe prestar especial atención a aquellos que controlan el crecimiento de los árboles, la velocidad de aprendizaje del modelo, y los que gestionan la parada temprana para evitar overfitting:\n",
    "\n",
    "   - `learning_rate`: tasa a la que aprende\n",
    "\n",
    "   - `max_iter`: número de árboles incluidos en el modelo.\n",
    "\n",
    "   - `max_depth`: profundidad máxima que pueden alcanzar los árboles.\n",
    "\n",
    "   - `random_state`: semilla para que los resultados sean reproducibles. Tiene que ser un valor entero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de hiperparámetros evaluados\n",
    "# ==============================================================================\n",
    "param_grid = {'learning_rate'    : [0.001, 0.01,0.1],\n",
    "              'max_depth'        : [1,3, 5],\n",
    "              'max_iter'         : [100, 300,500]\n",
    "             }\n",
    "\n",
    "# Búsqueda por grid search con validación cruzada\n",
    "# ==============================================================================\n",
    "grid = GridSearchCV(\n",
    "        estimator  = HistGradientBoostingRegressor(\n",
    "                        random_state        = 42,\n",
    "                        scoring             = 'loss',\n",
    "                    ),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'neg_root_mean_squared_error',\n",
    "        n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "        cv         = RepeatedKFold(n_splits=3, n_repeats=1, random_state=123), \n",
    "        refit      = True,\n",
    "        verbose    = 0,\n",
    "        return_train_score = True\n",
    "       )\n",
    "\n",
    "grid.fit(X = X_train, y = y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resultados\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "resultados.filter(regex = '(param.*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False) \\\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo final\n",
    "# ==============================================================================\n",
    "modelo_final = grid.best_estimator_\n",
    "predicciones = modelo_final.predict(X = X_test)\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca3bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
