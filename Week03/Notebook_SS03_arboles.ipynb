{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a952ad54-2218-4382-b2eb-d401d349f764",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"../banner/banner_ML_UNLP_1900_200.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523af81e",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/ML_UNLP_Lectures/blob/main/Week03/Notebook_SS03_arboles.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e57de4",
   "metadata": {},
   "source": [
    "# CARTs, Bagging, Random Forests y Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a3dcd",
   "metadata": {},
   "source": [
    "### Prediciendo precios de propiedades\n",
    "\n",
    "Nuestro objetivo hoy es construir un modelo para predecir los precios de la vivienda. Del art√≠culo hist√≥rico de Rosen \"Hedonic Prices and Implicit Markets: Product Differentiation in Pure Competition\" (1974), sabemos que un vector de sus caracter√≠sticas describe un bien diferenciado.\n",
    "\n",
    "En el caso de una casa, estas caracter√≠sticas pueden incluir atributos estructurales (por ejemplo, n√∫mero de dormitorios), amenidades, etc.. As√≠, podemos escribir el precio de mercado de la casa como:\n",
    "\n",
    "$$\n",
    "Price=f(atributos\\,estructurales,amenidades,...)\n",
    "$$\n",
    "\n",
    "\n",
    "Sin embargo, la teor√≠a de Rosen no nos dice mucho sobre la forma funcional de $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9183a2e",
   "metadata": {},
   "source": [
    "# CARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab5a6",
   "metadata": {},
   "source": [
    "Let's load the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6b3e2f",
   "metadata": {},
   "source": [
    " And the toy data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41492fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db= pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/toy_houses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=db, x='habitaciones', y='DCBD')\n",
    "\n",
    "# Scale x-axis and set labels\n",
    "plt.xticks(np.arange(0, 9, 1))\n",
    "plt.xlabel(\"Habitaciones\")\n",
    "plt.ylabel(\"Distancia al Centro\")\n",
    "\n",
    "# Apply classic theme\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Adjust text size and remove legend\n",
    "plt.rc('font', size=20)\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5668c",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "\n",
    "-  Datos: $y_{n\\times 1}$  y $X_{n\\times k}$ \n",
    "\n",
    "-  Definiciones\n",
    "\n",
    "      -  *j* es la variable que parte el espacio \n",
    "      - *s* es el punto de partici√≥n\n",
    "\n",
    "\n",
    "-  Definimos los siguientes semiplanos\n",
    "\n",
    "\\begin{align}\n",
    "R_1(j,s)=\\{X|X_j\\leq s\\} \\,\\,\\, \\& \\,\\,\\, R_2(j,s)=\\{X|X_j > s\\}\n",
    "\\end{align}\n",
    "\n",
    "-  *El problema*: buscar la variable de partici√≥n $X_j$ y el punto $s$ de forma tal que \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{j,s}{min} \\left[ \\underset{y_{R_1}}{min}\\sum_{x_i\\in R_1(j,s)}(y-y_{R_1})^2+ \\underset{y_{R_2}}{min}\\sum_{x_i\\in R_2(j,s)}(y-y_{R_2})^2\\right]\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872b7af",
   "metadata": {},
   "source": [
    "#### Algorithm by hand (\"artesanal\")\n",
    "\n",
    "1. Iniciemos por DBCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty list to store MSE values\n",
    "MSE_dbcd = []\n",
    "\n",
    "# Loop through the specified range\n",
    "for i in np.arange(1.25, 2, 0.25):\n",
    "    # Region 1\n",
    "    R1 = db[db['DCBD'] <= i]\n",
    "    yr1 = R1['price'].mean()\n",
    "    MSEr1 = ((R1['price'] - yr1) ** 2).mean() \n",
    "\n",
    "    # Region 2\n",
    "    R2 = db[db['DCBD'] > i]\n",
    "    yr2 = R2['price'].mean()\n",
    "    MSEr2 = ((R2['price'] - yr2) ** 2).mean()\n",
    "\n",
    "    # Store the sum of MSEs\n",
    "    MSE_dbcd.append(MSEr1 + MSEr2)\n",
    "\n",
    "# Output the result\n",
    "MSE_dbcd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1a57",
   "metadata": {},
   "source": [
    "2. Luego por Habitaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty list to store MSE values\n",
    "MSE_hab = []\n",
    "\n",
    "# Loop through the specified range\n",
    "for i in range(9):  # 0 to 8 inclusive\n",
    "    # Region 1\n",
    "    R1 = db[db['habitaciones'] <= i]\n",
    "    yr1 = R1['price'].mean()\n",
    "    MSEr1 = ((R1['price'] - yr1) ** 2).mean() \n",
    "\n",
    "    # Region 2\n",
    "    R2 = db[db['habitaciones'] > i]\n",
    "    yr2 = R2['price'].mean()\n",
    "    MSEr2 = ((R2['price'] - yr2) ** 2).mean() \n",
    "\n",
    "    # Store the sum of MSEs\n",
    "    MSE_hab.append(MSEr1 + MSEr2)\n",
    "\n",
    "# Output the result\n",
    "MSE_hab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd652993",
   "metadata": {},
   "source": [
    "**M√≠nimo?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f0a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the MSE lists\n",
    "MSE = MSE_dbcd + MSE_hab\n",
    "\n",
    "# Find the minimum MSE and its index\n",
    "min_MSE = min(MSE)\n",
    "min_MSE_index = MSE.index(min_MSE) + 1  # Adding 1 because Python indexing starts at 0\n",
    "\n",
    "# Output the minimum MSE and its index\n",
    "print(\"Minimum MSE is at index:\", min_MSE_index, \"with a value of:\", min_MSE)\n",
    "\n",
    "# Output the entire MSE list\n",
    "print(\"MSE values:\", MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29393e0",
   "metadata": {},
   "source": [
    "#### Algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8269f",
   "metadata": {},
   "source": [
    "La principal implementaci√≥n de √°rboles de regresi√≥n en `Python` est√° disponible en la librer√≠a `scikit-learn` a trav√©s de las clase `DecisionTreeRegressor`. Una caracter√≠stica importante para aquellos que han utilizado otras implementaciones es que, en `scikit-learn`, es necesario convertir las variables categ√≥ricas en variables dummy (one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69588256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Prepare the features (X) and target (y)\n",
    "X = db[['DCBD', 'habitaciones']]\n",
    "y = db['price']\n",
    "\n",
    "# Create the decision tree model\n",
    "mytree = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "\n",
    "# Fit the model\n",
    "mytree.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a095abd",
   "metadata": {},
   "source": [
    "Una vez entrenado el √°rbol, se puede representar mediante la combinaci√≥n de las funciones  `export_text()` y `plot_tree()`. \n",
    " - La funci√≥n `export_text()` la estructura del √°rbol  y valor medio de la variable respuesta en cada nodo.  \n",
    " - La funci√≥n `plot_tree()` dibuja la estructura del √°rbol y muestra el n√∫mero de observaciones y valor medio de la variable respuesta en cada nodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "texto_modelo = export_text(\n",
    "                    decision_tree = mytree,\n",
    "                    feature_names = list(X.columns)\n",
    "               )\n",
    "print(texto_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "\n",
    "# Estructura del √°rbol creado\n",
    "# ------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "print(f\"Profundidad del √°rbol: {mytree.get_depth()}\")\n",
    "print(f\"N√∫mero de nodos terminales: {mytree.get_n_leaves()}\")\n",
    "\n",
    "plot = plot_tree(\n",
    "            decision_tree = mytree,\n",
    "            feature_names = X.columns,\n",
    "            class_names   = 'price',\n",
    "            filled        = True,\n",
    "            impurity      = False,\n",
    "            fontsize      = 10,\n",
    "            precision     = 2,\n",
    "            ax            = ax\n",
    "       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08f3fa",
   "metadata": {},
   "source": [
    "##### With California housing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing=pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/california_housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22abd8a5",
   "metadata": {},
   "source": [
    "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "\n",
    "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "\n",
    "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "\n",
    "4. totalRooms: Total number of rooms within a block\n",
    "\n",
    "5. totalBedrooms: Total number of bedrooms within a block\n",
    "\n",
    "6. population: Total number of people residing within a block\n",
    "\n",
    "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "\n",
    "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "\n",
    "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "\n",
    "10. oceanProximity: Location of the house w.r.t ocean/sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c48aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d98ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the missing data\n",
    "california_housing = california_housing.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data\n",
    "X = california_housing.drop('median_house_value', axis = 1)  # Features\n",
    "Y = california_housing['median_house_value']                 # Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree_california_1 = DecisionTreeRegressor(max_leaf_nodes = 8).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bbd918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporing the categorical data\n",
    "california_housing['ocean_proximity'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical values to numeric values using one-hot encoding\n",
    "california_housing = pd.get_dummies(california_housing, columns= ['ocean_proximity'])\n",
    "\n",
    "# Showing the data after Converting categorical values to numeric values\n",
    "california_housing.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7074c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data\n",
    "X2 = california_housing.drop('median_house_value', axis = 1)  # Features\n",
    "Y = california_housing['median_house_value']                 # Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c555aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_california_2 = DecisionTreeRegressor(max_leaf_nodes = 4)\n",
    "\n",
    "tree_california_2.fit(X2,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura del √°rbol creado\n",
    "# ------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "print(f\"Profundidad del √°rbol: {tree_california_2.get_depth()}\")\n",
    "print(f\"N√∫mero de nodos terminales: {tree_california_2.get_n_leaves()}\")\n",
    "\n",
    "plot = plot_tree(\n",
    "            decision_tree = tree_california_2,\n",
    "            feature_names = X2.columns,\n",
    "            class_names   = 'median_house_value',\n",
    "            filled        = True,\n",
    "            impurity      = False,\n",
    "            fontsize      = 10,\n",
    "            precision     = 2,\n",
    "            ax            = ax\n",
    "       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10c96b",
   "metadata": {},
   "source": [
    "### Sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cce89d",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"figures/tree_uba.png\"  width=\"300\" height=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f410da",
   "metadata": {},
   "source": [
    "\n",
    "`DecisionTreeRegressor` del m√≥dulo `sklearn.tree` tiene  hiperpar√°metros, que nos permitir√°n \"controlar\" el sobreajuste:\n",
    "\n",
    "  - `max_depth`: profundidad m√°xima que puede alcanzar el √°rbol.\n",
    "\n",
    "  - `max_leaf_nodes`: n√∫mero m√°ximo de nodos terminales.\n",
    "\n",
    "  - `min_samples_leaf`: n√∫mero m√≠nimo de observaciones que debe de tener cada uno de los nodos hijos para que se produzca la divisi√≥n. Si es un valor decimal se interpreta como fracci√≥n del total de observaciones de entrenamiento `ceil(min_samples_split * n_samples)`.\n",
    "\n",
    "  \n",
    "  - `ccp_alpha`=0.0 Cost complexity prunning\n",
    "  \n",
    "  - `random_state`: semilla para que los resultados sean reproducibles. Tiene que ser un valor entero.\n",
    "\n",
    "Como en todo estudio de regresi√≥n, no solo es importante ajustar el modelo, sino tambi√©n cuantificar su capacidad para predecir nuevas observaciones. Para poder hacer la posterior evaluaci√≥n, se dividen los datos en dos grupos, uno de entrenamiento y otro de test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n de los datos en train y test\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X2,\n",
    "                                        Y,\n",
    "                                        test_size=0.3,\n",
    "                                        random_state = 1010101\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddbde6",
   "metadata": {},
   "source": [
    "#### Fijar la profundidad del √°rbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "param_grid = {'max_depth': [1, 5, 10, 15,20]}\n",
    "\n",
    "# B√∫squeda por validaci√≥n cruzada\n",
    "grid_depth = GridSearchCV(\n",
    "        # El √°rbol se crece al m√°ximo posible para luego aplicar el pruning\n",
    "        estimator = DecisionTreeRegressor(random_state  = 123),\n",
    "        param_grid = param_grid,\n",
    "        cv         = 5,\n",
    "        refit      = True,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        return_train_score = True\n",
    "      )\n",
    "\n",
    "grid_depth.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb052b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor valor max_depth encontrado\n",
    "# ------------------------------------------------------------------------------\n",
    "grid_depth.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "scores = pd.DataFrame(grid_depth.cv_results_)\n",
    "scores.plot(x='param_max_depth', y='mean_train_score', ax=ax)\n",
    "scores.plot(x='param_max_depth', y='mean_test_score', ax=ax)\n",
    "ax.set_title(\"Error de validacion cruzada vs hiperpar√°metro max_depth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135403a9",
   "metadata": {},
   "source": [
    "#### Cost complexity Prunning\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "  C_{\\alpha}(T)= \\sum_{m=1}^{[T]}  \\sum_{x_i\\in R_m} (y_i-\\hat{y}_m)^2 + \\alpha [T]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "##### Algoritmo completo\n",
    "\n",
    "  - Hacemos crecer el √°rbol\n",
    "\n",
    "  - Para un dado $\\alpha$, aplicamos  *cost complexity pruning* \n",
    "    \n",
    "  - Utilizamos K-fold cross-validation para elegir $\\alpha$. \n",
    "\n",
    "  \n",
    "Tenemos entonces una secuencia de subarboles para distintos valores de $\\alpha$ \n",
    "\n",
    "Elegimos el $\\alpha$ y el sub√°rbol que tienen el menor error de predicci√≥n.\n",
    "\n",
    "\n",
    "\n",
    "Para aplicar el proceso de pruning en `scikit-learn` es necesario indicar el argumento `ccp_alpha` que determina el grado de penalizaci√≥n por complejidad. Cuanto mayor es este valor, m√°s agresivo el podado y menor el tama√±o del √°rbol resultante. Dado que no hay forma de conocer de antemano el valor √≥ptimo de `ccp_alpha`, se recurre a validaci√≥n cruzada para identificarlo.\n",
    "\n",
    "Aunque existen otras formas de indentificar √°rboles \"optimos\", por ejemplo identificando el valor de `max_depth` y `min_samples_split` mediante validaci√≥n cruzada, el pruning puede generar mejores resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a80471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning (const complexity pruning) por validaci√≥n cruzada\n",
    "# ------------------------------------------------------------------------------\n",
    "# Valores de ccp_alpha evaluados\n",
    "param_grid = {'ccp_alpha':np.linspace(1000000, 10000000, 10)}\n",
    "\n",
    "# B√∫squeda por validaci√≥n cruzada\n",
    "grid = GridSearchCV(\n",
    "        # El √°rbol se crece al m√°ximo posible para luego aplicar el pruning\n",
    "        estimator = DecisionTreeRegressor(\n",
    "                            max_depth         = None,\n",
    "                            min_samples_split = 2,\n",
    "                            min_samples_leaf  = 1,\n",
    "                            random_state      = 123\n",
    "                       ),\n",
    "        param_grid = param_grid,\n",
    "        cv         = 5,\n",
    "        refit      = True,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        return_train_score = True\n",
    "      )\n",
    "\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eacddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor valor ccp_alpha encontrado\n",
    "# ------------------------------------------------------------------------------\n",
    "grid.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ce332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "scores = pd.DataFrame(grid.cv_results_)\n",
    "scores.plot(x='param_ccp_alpha', y='mean_train_score', ax=ax)\n",
    "scores.plot(x='param_ccp_alpha', y='mean_test_score', ax=ax)\n",
    "ax.set_title(\"Error de validacion cruzada vs hiperpar√°metro ccp_alpha\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70ca1c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Una vez identificado el valor √≥ptimo de `ccp_alpha`, se reentrena el √°rbol indicando este valor en sus argumentos. Si en el `GridSearchCV()` se indica `refit=True`, este reentrenamiento se hace autom√°ticamente y el modelo resultante se encuentra almacenado en `.best_estimator_`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c54fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura del √°rbol final\n",
    "# ------------------------------------------------------------------------------\n",
    "modelo_prunning = grid.best_estimator_\n",
    "print(f\"Profundidad del √°rbol: {modelo_prunning.get_depth()}\")\n",
    "print(f\"N√∫mero de nodos terminales: {modelo_prunning.get_n_leaves()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382834c6",
   "metadata": {},
   "source": [
    "#### Predicci√≥n  y evaluaci√≥n del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a7e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo max_depth\n",
    "#-------------------------------------------------------------------------------\n",
    "modelo_depth = grid_depth.best_estimator_\n",
    "predicciones = modelo_depth.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo final (tras aplicar pruning)\n",
    "#-------------------------------------------------------------------------------\n",
    "predicciones = modelo_prunning.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ba4df",
   "metadata": {},
   "source": [
    "### Comentarios sobre √Årboles\n",
    "\n",
    "\n",
    "#### Pros: \n",
    "  \n",
    "    - Los √°rboles son muy f√°ciles de explicar a las personas (probablemente incluso m√°s f√°ciles que la regresi√≥n lineal)\n",
    "\n",
    "    - Los √°rboles se pueden trazar gr√°ficamente y son f√°cilmente interpretados incluso por no expertos. Variables m√°s importantes en la parte superior\n",
    "\n",
    "\n",
    "\n",
    "#### Cons:\n",
    "    \n",
    "    - Si la estructura es lineal, CART no funciona bien\n",
    "    \n",
    "<div >\n",
    "<img src = \"figures/tree_vs_reg.png\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "    - Los √°rboles no son muy robustos \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1a513",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaci√≥n del modelo\n",
    "# ==============================================================================\n",
    "modelo_forest = RandomForestRegressor(\n",
    "            n_estimators = 10,\n",
    "            criterion    = 'squared_error',\n",
    "            max_depth    = None,\n",
    "            max_features = 1,\n",
    "            oob_score    = False,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 123\n",
    "         )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "# ==============================================================================\n",
    "modelo_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a55c9",
   "metadata": {},
   "source": [
    "### Predicci√≥n y evaluaci√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e40992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo inicial\n",
    "# ==============================================================================\n",
    "predicciones_forest = modelo_forest.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones_forest,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b92de",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Optimizaci√≥n de hiperpar√°metros\n",
    "\n",
    "El modelo inicial se ha entrenado utilizando 10 √°rboles (n_estimators=10) y manteniendo el resto de hiperpar√°metros con su valor por defecto. Al ser hiperpar√°metros, no se puede saber de antemano cu√°l es el valor m√°s adecuado, la forma de identificarlos es mediante el uso de estrategias de validaci√≥n, por ejemplo validaci√≥n cruzada.\n",
    "\n",
    "Los modelos Random Forest tienen la ventaja de disponer del Out-of-Bag error, lo que permite obtener una estimaci√≥n del error de test sin recurrir a la validaci√≥n cruzada, que es computacionalmente costosa. En la implementaci√≥n de RandomForestRegressor, la m√©trica devuelta como oob_score es el ùëÖ2, si se desea otra, se tiene que recurrir al m√©todo oob_decision_function_() para obtener las predicciones y con ellas calcular la m√©trica de inter√©s. Para una explicaci√≥n m√°s detallada consultar: Grid search de modelos Random Forest con out-of-bag error y early stopping.\n",
    "\n",
    "Cabe tener en cuenta que, cuando se busca el valor √≥ptimo de un hiperpar√°metro con dos m√©tricas distintas, el resultado obtenido raramente es el mismo. Lo importante es que ambas m√©tricas identifiquen las mismas regiones de inter√©s.\n",
    "\n",
    "\n",
    "\n",
    "   La clase RandomForestRegressor del m√≥dulo sklearn.ensemble contiene varios hiperpar√°metros. De entre todos ellos, destacan aquellos que detienen el crecimiento de los √°rboles, los que controlan el n√∫mero de √°rboles y predictores incluidos, y los que gestionan la paralelizaci√≥n:\n",
    "\n",
    "   - `n_estimators`; n√∫mero de √°rboles incluidos en el modelo.\n",
    "\n",
    "   - `max_depth`: profundidad m√°xima que pueden alcanzar los √°rboles.\n",
    "\n",
    "   - `min_samples_split`: n√∫mero m√≠nimo de observaciones que debe de tener un nodo para que pueda dividirse. Si es un valor decimal se interpreta como fracci√≥n del total de observaciones de entrenamiento ceil(min_samples_split * n_samples).\n",
    "\n",
    "   - `min_samples_leaf`: n√∫mero m√≠nimo de observaciones que debe de tener cada uno de los nodos hijos para que se produzca la divisi√≥n. Si es un valor decimal se interpreta como fracci√≥n del total de observaciones de entrenamiento ceil(min_samples_split * n_samples).\n",
    "\n",
    "   - `max_leaf_nodes`: n√∫mero m√°ximo de nodos terminales que pueden tener los √°rboles.\n",
    "\n",
    "   - `max_features`: n√∫mero de predictores considerados a en cada divisi√≥n. Puede ser:\n",
    "        Un valor entero\n",
    "        Una fracci√≥n del total de predictores..\n",
    "        ‚Äúsqrt‚Äù, raiz cuadrada del n√∫mero total de predictores.\n",
    "        ‚Äúlog2‚Äù, log2 del n√∫mero total de predictores.\n",
    "        None, utiliza todos los predictores.\n",
    "\n",
    "   - `oob_score`: Si se calcula o no el out-of-bag R^2. Por defecto es False ya que aumenta el tiempo de entrenamiento.\n",
    "\n",
    "   - `n_jobs`: n√∫mero de cores empleados para el entrenamiento. En random forest los √°rboles se ajustan de forma independiente, por lo la paralelizaci√≥n reduce notablemente el tiempo de entrenamiento. Con -1 se utilizan todos los cores disponibles.\n",
    "\n",
    "   - `random_state`: semilla para que los resultados sean reproducibles. Tiene que ser un valor entero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386aa52b",
   "metadata": {},
   "source": [
    "#### N√∫mero de √°rboles\n",
    "\n",
    "En Random Forest, el n√∫mero de √°rboles no es un hiperpar√°metro cr√≠tico en cuanto que, a√±adir √°rboles, solo puede hacer que mejorar el resultado. En Random Forest no se produce overfitting por exceso de √°rboles. Sin embargo, a√±adir √°rboles una vez que la mejora se estabiliza es una perdida te recursos computacionales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e876df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaci√≥n empleando el Out-of-Bag error\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train_scores = []\n",
    "oob_scores   = []\n",
    "\n",
    "# Valores evaluados\n",
    "estimator_range = range(10, 150, 5)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de n_estimators y extraer su error\n",
    "# de entrenamiento y de Out-of-Bag.\n",
    "for n_estimators in estimator_range:\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = n_estimators,\n",
    "                criterion    = 'squared_error',\n",
    "                max_depth    = None,\n",
    "                max_features = 1,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 123\n",
    "             )\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(modelo.score(X_train, y_train))\n",
    "    oob_scores.append(modelo.oob_score_)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fa8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Gr√°fico con la evoluci√≥n de los errores\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "ax.plot(estimator_range, oob_scores, label=\"out-of-bag scores\")\n",
    "ax.plot(estimator_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "        marker='o', color = \"red\", label=\"max score\")\n",
    "ax.set_ylabel(\"R^2\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_title(\"Evoluci√≥n del out-of-bag-error vs n√∫mero √°rboles\")\n",
    "plt.legend();\n",
    "print(f\"Valor √≥ptimo de n_estimators: {estimator_range[np.argmax(oob_scores)]}\")\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123461b",
   "metadata": {},
   "source": [
    "#### Max features\n",
    "\n",
    "El valor de `max_features` es uno de los hiperpar√°metros m√°s importantes de random forest, ya que es el que permite controlar cu√°nto se decorrelacionan los √°rboles entre s√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing dataset correlation in heat map\n",
    "housing_dataset_correlation = california_housing.corr()\n",
    "plt.figure(figsize=(24,14))\n",
    "sns.heatmap(housing_dataset_correlation, annot = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaci√≥n empleando el Out-of-Bag error\n",
    "# ==============================================================================\n",
    "train_scores = []\n",
    "oob_scores   = []\n",
    "\n",
    "# Valores evaluados\n",
    "max_features_range = range(1, X_train.shape[1] + 1, 1)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de max_features y extraer su error\n",
    "# de entrenamiento y de Out-of-Bag.\n",
    "for max_features in max_features_range:\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = 100,\n",
    "                criterion    = 'squared_error',\n",
    "                max_depth    = None,\n",
    "                max_features = max_features,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 123\n",
    "             )\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(modelo.score(X_train, y_train))\n",
    "    oob_scores.append(modelo.oob_score_)\n",
    "    \n",
    "# Gr√°fico con la evoluci√≥n de los errores\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.plot(max_features_range, train_scores, label=\"train scores\")\n",
    "ax.plot(max_features_range, oob_scores, label=\"out-of-bag scores\")\n",
    "ax.plot(max_features_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "        marker='o', color = \"red\")\n",
    "ax.set_ylabel(\"R^2\")\n",
    "ax.set_xlabel(\"max_features\")\n",
    "ax.set_title(\"Evoluci√≥n del out-of-bag-error vs n√∫mero de predictores\")\n",
    "plt.legend();\n",
    "print(f\"Valor √≥ptimo de max_features: {max_features_range[np.argmax(oob_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afc45a",
   "metadata": {},
   "source": [
    "##### Grid search\n",
    "\n",
    "Aunque el an√°lisis individual de los hiperpar√°metros es √∫til para entender su impacto en el modelo e identificar rangos de inter√©s, la b√∫squeda final no debe hacerse de forma secuencial, ya que cada hiperpar√°metro interacciona con los dem√°s. Es preferible recurrir a grid search o random search para analizar varias combinaciones de hiperpar√°metros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f884326",
   "metadata": {},
   "source": [
    "<div >\n",
    "<img src = \"figures/gridsearch.png\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe062f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Aleatorio basado en validaci√≥n cruzada\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "\n",
    "# Grid de hiperpar√°metros evaluados\n",
    "# ==============================================================================\n",
    "param_grid = {'n_estimators': [100,150,200],\n",
    "              'max_features': [5, 7, 9],\n",
    "              'max_depth'   : [None, 3, 10, 20]\n",
    "             }\n",
    "\n",
    "# B√∫squeda por grid search con validaci√≥n cruzada\n",
    "# ==============================================================================\n",
    "grid_forest_cv = RandomizedSearchCV(\n",
    "            estimator  = RandomForestRegressor(random_state = 123),\n",
    "            param_distributions = param_grid,\n",
    "            scoring    = 'neg_root_mean_squared_error',\n",
    "            n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "            n_iter=2, \n",
    "            cv=5, \n",
    "            random_state=42,\n",
    "            refit=True\n",
    "       )\n",
    "\n",
    "grid_forest_cv.fit(X = X_train, y = y_train)\n",
    "\n",
    "# Resultados\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid_forest_cv.cv_results_)\n",
    "resultados.filter(regex = '(param.*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False) \\\n",
    "    .head(4)\n",
    "\n",
    "\n",
    "# Mejores hiperpar√°metros por validaci√≥n cruzada\n",
    "# ==============================================================================\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Mejores hiperpar√°metros encontrados (cv)\")\n",
    "print(\"----------------------------------------\")\n",
    "print(grid_forest_cv.best_params_, \":\", grid_forest_cv.best_score_, grid_forest_cv.scoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1450d30",
   "metadata": {},
   "source": [
    "Una vez identificados los mejores hiperpar√°metros, se reentrena el modelo indicando los valores √≥ptimos en sus argumentos. Si en el GridSearchCV() se indica refit=True, este reentrenamiento se hace autom√°ticamente y el modelo resultante se encuentra almacenado en .best_estimator_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c085161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo final\n",
    "# ==============================================================================\n",
    "modelo_grid_forest_cv_final = grid_forest_cv.best_estimator_\n",
    "predicciones_grid_forest_cv_final = modelo_grid_forest_cv_final.predict(X = X_test)\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones_grid_forest_cv_final,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1732bc",
   "metadata": {},
   "source": [
    "# Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec803c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(\"https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/boosting_tree_toy.csv\")\n",
    "\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=db['y']\n",
    "x=db['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "\n",
    "# Adding labels and title for clarity\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=db['x'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a69ecb",
   "metadata": {},
   "source": [
    "### Hiperpar√°metros\n",
    "\n",
    "- $\\lambda$ la tasa a la que aprende, los valores t√≠picos son 0.1, 0.01 o 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam =0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4219b8",
   "metadata": {},
   "source": [
    "- Tama√±o del √°rbol. Arboles pocos profundos  funcionan bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66339a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=1 #stump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab704f53",
   "metadata": {},
   "source": [
    "- Iniciamos fijando $\\hat{f}(x)=0$ y $r_i=y_i$ para todos los $i$ del training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhat = np.zeros(len(y))\n",
    "\n",
    "r=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cafcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f8fd9",
   "metadata": {},
   "source": [
    "Para $m=1,2,...,M$\n",
    "\n",
    " - Ajustamos un √°rbol $\\hat{f}^m$ con $d$ bifurcaciones ($d+1$ hojas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primera iteraci√≥n\n",
    "model = DecisionTreeRegressor(max_depth=d, random_state=12)\n",
    "model.fit(x, y)\n",
    "\n",
    "  # Make predictions\n",
    "yhat1 = model.predict(x)\n",
    "\n",
    "\n",
    "lam *yhat1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55ac8f",
   "metadata": {},
   "source": [
    "   - Actualizamos $\\hat{f}(x)$ con una versi√≥n \"shrunken\" del nuevo √°rbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f776039",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=fhat + lam *yhat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6584eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of x vs y\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Add a step line plot of x vs f1\n",
    "plt.step(x, f1, where='mid', color='red', linewidth=3)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38170e6",
   "metadata": {},
   "source": [
    "- Actualizamos los residuales\n",
    "  \\begin{align}\n",
    "  r_i\\leftarrow r_i-\\lambda\\hat{f}^m(x)\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92889de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "r1 = r - lam*yhat1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208fefa",
   "metadata": {},
   "source": [
    "El loop vuelve a iniciar, en la iteraci√≥n 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteracion 2\n",
    "\n",
    "model.fit(x, r1)\n",
    "\n",
    "  # Make predictions\n",
    "yhat2 = model.predict(x)\n",
    "\n",
    "\n",
    "\n",
    "f2 = f1 + lam *yhat2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a79928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.step(x, f2, where='mid', color='red', linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09734c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En un loop\n",
    "# Initialize fhat and residuals (r)\n",
    "fhat = np.zeros(len(y))\n",
    "r = y.copy()\n",
    "\n",
    "# Set lambda (adjust as needed)\n",
    "lambda_ = 0.1\n",
    "\n",
    "# Initialize YP with lambda * fhat\n",
    "YP = lambda_ * fhat.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(500):\n",
    "    # Fit a decision tree regressor\n",
    "    fit = DecisionTreeRegressor(max_depth=1)\n",
    "    fit.fit(x.reshape(-1, 1), r)\n",
    "\n",
    "    # Predict using the fitted model\n",
    "    yhat = fit.predict(x.reshape(-1, 1))\n",
    "\n",
    "    # Update residuals\n",
    "    r = r - lambda_ * yhat\n",
    "\n",
    "    # Update YP\n",
    "    YP = np.hstack((YP, (lambda_ * yhat).reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100\n",
    "fhat = np.apply_along_axis(np.sum, 1, YP[:, :M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb76c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of x vs y\n",
    "plt.scatter(x, y, label='Data')\n",
    "\n",
    "# Step plot of x vs fhat\n",
    "plt.step(x, fhat, where='mid', color='red', linewidth=3, label='Boosted Model')\n",
    "\n",
    "# Setting labels and showing the plot\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa7ab6",
   "metadata": {},
   "source": [
    "## Con los datos de California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6929793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Creaci√≥n del modelo\n",
    "# ==============================================================================\n",
    "modelo_boost = HistGradientBoostingRegressor(\n",
    "            max_iter     = 600,\n",
    "            loss         = 'squared_error',\n",
    "            random_state = 123\n",
    "         )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "# ==============================================================================\n",
    "modelo_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Error de test del modelo inicial\n",
    "# ==============================================================================\n",
    "predicciones_boost = modelo_boost.predict(X = X_test)\n",
    "\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones_boost,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a564c0a",
   "metadata": {},
   "source": [
    "\n",
    "#### Ajuste del modelo, Hiperpar√°metros y Grid Search basado en validaci√≥n cruzada\n",
    "\n",
    "Puede encontrarse una descripci√≥n detallada de todos ellos en sklearn.ensemble.GradientBoostingRegressor. En la pr√°ctica, cabe prestar especial atenci√≥n a aquellos que controlan el crecimiento de los √°rboles, la velocidad de aprendizaje del modelo, y los que gestionan la parada temprana para evitar overfitting:\n",
    "\n",
    "   - `learning_rate`: tasa a la que aprende\n",
    "\n",
    "   - `max_iter`: n√∫mero de √°rboles incluidos en el modelo.\n",
    "\n",
    "   - `max_depth`: profundidad m√°xima que pueden alcanzar los √°rboles.\n",
    "\n",
    "   - `random_state`: semilla para que los resultados sean reproducibles. Tiene que ser un valor entero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de hiperpar√°metros evaluados\n",
    "# ==============================================================================\n",
    "param_grid = {'learning_rate'    : [0.001, 0.01,0.1],\n",
    "              'max_depth'        : [1,3, 5],\n",
    "              'max_iter'         : [100, 300,500]\n",
    "             }\n",
    "\n",
    "# B√∫squeda por grid search con validaci√≥n cruzada\n",
    "# ==============================================================================\n",
    "grid = GridSearchCV(\n",
    "        estimator  = HistGradientBoostingRegressor(\n",
    "                        random_state        = 42,\n",
    "                        scoring             = 'loss',\n",
    "                    ),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'neg_root_mean_squared_error',\n",
    "        n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "        cv         = RepeatedKFold(n_splits=3, n_repeats=1, random_state=123), \n",
    "        refit      = True,\n",
    "        verbose    = 0,\n",
    "        return_train_score = True\n",
    "       )\n",
    "\n",
    "grid.fit(X = X_train, y = y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resultados\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "resultados.filter(regex = '(param.*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('mean_test_score', ascending = False) \\\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo final\n",
    "# ==============================================================================\n",
    "modelo_final = grid.best_estimator_\n",
    "predicciones = modelo_final.predict(X = X_test)\n",
    "rmse = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "        squared = False\n",
    "       )\n",
    "print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca3bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
