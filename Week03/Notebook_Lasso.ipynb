{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a952ad54-2218-4382-b2eb-d401d349f764",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"../banner/banner_ML_UNLP_1900_200.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208acc15",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/ML_UNLP_Lectures/blob/main/Week03/Notebook_Lasso.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e57de4",
   "metadata": {},
   "source": [
    "\n",
    "# Regularization: Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a3dcd",
   "metadata": {},
   "source": [
    "## Predicting Wages\n",
    "\n",
    "Our objective today is to construct a model of individual wages\n",
    "\n",
    "$$\n",
    "w = f(X) + u \n",
    "$$\n",
    "\n",
    "where w is the  wage, and X is a matrix that includes potential explanatory variables/predictors. In this problem set, we will focus on a linear model of the form\n",
    "\n",
    "\\begin{align}\n",
    " ln(w) & = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p  + u \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ba2f4",
   "metadata": {},
   "source": [
    "were $ln(w)$ is the logarithm of the wage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab5a6",
   "metadata": {},
   "source": [
    "Let's load the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85f424",
   "metadata": {},
   "source": [
    "and the data set that is a sample of the NLSY97. The NLSY97 is  a nationally representative sample of 8,984 men and women born during the years 1980 through 1984 and living in the United States at the time of the initial survey in 1997.  Participants were ages 12 to 16 as of December 31, 1996.  Interviews were conducted annually from 1997 to 2011 and biennially since then.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6979f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlsy = pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/nlsy97.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94ec2b-ab94-4ec8-8512-e370d2f22a56",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "X1 = nlsy[[ \"educ\", \"exp\", \"afqt\", \"mom_educ\", \"dad_educ\"]]\n",
    "X2=nlsy.drop(columns=['lnw_2016'])\n",
    "y=nlsy['lnw_2016']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d73d77",
   "metadata": {},
   "source": [
    "We want to construct a model that predicts well out of sample, and we have potentially 994 regressors. We are going to regularize this regression using Ridge.\n",
    "\n",
    "## Lasso\n",
    "\n",
    "We first illustrate ridge regression, which can be fit using `sklearn` and seeks to minimize\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij}    \\right) ^ 2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| .\n",
    "$$\n",
    "\n",
    "Notice that the intercept is not penalized. \n",
    "\n",
    "\n",
    "Lasso penalizes the absolute value  of the coefficients. As a result, lasso shrinks coefficients toward zero all the way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82add41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "?Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5df971",
   "metadata": {},
   "source": [
    "The Lasso() function has an alpha argument (λ, but with a different name!) that is used to tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b42d3",
   "metadata": {},
   "source": [
    "Let's run the ridge regression (we need to set the parameter `alpha` to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = 0)\n",
    "\n",
    "lasso.fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lm=linear_model.LinearRegression().fit(X1,y)\n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = .11)\n",
    "\n",
    "lasso.fit(X1, y)\n",
    "\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1756cf3",
   "metadata": {},
   "source": [
    " We'll generate an array of alpha values ranging from very big to very small, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74431243",
   "metadata": {},
   "source": [
    "Associated with each alpha value is a vector of ridge regression coefficients, which we'll store in a matrix coefs. In this case, it is a 5×100 matrix, with 19 rows (one for each predictor) and 100 columns (one for each value of alpha). Remember that we'll want to standardize the variables so that they are on the same scale. To do this, we can use the normalize = True parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Lasso()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X1, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "np.shape(coefs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46568ae7",
   "metadata": {},
   "source": [
    "Let's see how  how much the coefficients are penalized for different values of $\\lambda$. Notice none of the coefficients are forced to be zero, although they get close to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha/lambda')\n",
    "plt.ylabel('Coefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84285ef",
   "metadata": {},
   "source": [
    "#### All the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha = a)\n",
    "    lasso.fit(X2, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha/lambda')\n",
    "plt.ylabel('Coefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82f59e",
   "metadata": {},
   "source": [
    "## Penalty selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be031d15",
   "metadata": {},
   "source": [
    "Instead of arbitrarily choosing , it would be better to use cross-validation to choose the tuning parameter alpha. We can do this using the cross-validated lasso regression function, `LassoCV()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad989ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassocv = LassoCV(alphas = alphas, scoring = 'neg_mean_squared_error')\n",
    "lassocv.fit(X2, y)\n",
    "lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84307874",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_star = Lasso(alpha = ridgecv.alpha_)\n",
    "lasso_cv_star.fit(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv_star.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909307ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
