---
author:
- 'Ignacio Sarmiento-Barbieri'
subtitle: Machine Learning
title: Selección de Modelos y Regularización
---

### Agenda

Recap: Predicción y Overfit
===========================

### Recap: Predicción y Overfit

-   Last Week:

    -   Machine Learning is all about prediction

    -   ML targets something different than causal inference, they can
        complement each other

    -   Bias Variance trade-off: tolerating some bias is possible to
        reduce V($\hat f(X)$) and lower MSE (ML best kept secret)

    -   Overfit and Model Selecction

        -   AIC y BIC

        -   Validation Approach

        -   LOOCV

        -   K-fold Cross-Validation

### Recap: Train and Test Sets. In-Sample and Out-of-Sample Prediction.

-   El objetivo es predecir $y$ dadas otras variables $X$. Ej: salario
    dadas las características del individuo

-   Asumimos que el link entre $y$ and $X$ esta dado por el modelo:

    $$\begin{aligned}
        y = f(X) + u
      \end{aligned}$$

-   donde $f(X)$ por ejemplo es
    $\beta_0 + \beta_1 X_1 + \dots + \beta_k X_k$

-   $u$ una variable aleatoria no observable $E(u)=0$ and
    $V(u) = \sigma^2$

### Recap: In-Sample Prediction and Overfit

![image](figures/fig1.pdf)

### Recap: In-Sample Prediction and Overfit

![image](figures/fig2.pdf)

### Recap: In-Sample Prediction and Overfit

![image](figures/fig3.pdf)

### Recap: In-Sample Prediction and Overfit

![image](figures/fig4.pdf)

### Recap: In-Sample Prediction and Overfit

![image](figures/train_test_error0.png)

\<1\>\[label=out\]

### Recap: Out-of-Sample Prediction and Overfit

-   ML nos interesa la predicción fuera de muestra

-   Overfit: modelos complejos predicen muy bien dentro de muestra, pero
    tienden a hacer un mal trabajo fuera de muestra

-   Hay que elegir el nivel adecuado de complejidad

### Recap: Out-of-Sample Prediction and Overfit

![image](figures/fig5.pdf)

### Recap: Out-of-Sample Prediction and Overfit

![image](figures/fig6.pdf)

### Recap: Overfit y Predicción fuera de Muestra

![image](figures/train_test_error.png)

### Recap: Overfit y Predicción fuera de Muestra

-   ML nos interesa la predicción fuera de muestra

-   Overfit: modelos complejos predicen muy bien dentro de muestra, pero
    tienden a hacer un mal trabajo fuera de muestra

-   Hay que elegir el modelo que "mejor" prediga fuera de muestra
    (out-of-sample)

    -   Penalización ex-post: AIC, BIC, R2 ajustado, etc

    -   Métodos de Remuestreo

        -   Enfoque del conjunto de validación

        -   LOOCV

        -   Validación cruzada en K-partes (5 o 10)

Selección de Modelos
====================

### Model Subset Selection

-   We have $M_k$ models

-   We want to find the model that best predicts out of sample

-   We have a number of ways to go about it

    -   Best Subset Selection

    -   Stepwise Selection

        -   Forward selection

        -   Backward selection

### Best Subset Selection

$$\begin{aligned}
  y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p +u\end{aligned}$$

1.  Estimate ***all*** possible models with $k=0,1,..., p$ predictors.

2.  Compute the prediction error using cross validation

3.  Pick the one with the smallest prediction error

Best Subset Selection
---------------------

### Best Subset Selection

1.  Let $M_0$ denote the null model, which contains no predictors. This
    model simply predicts the sample mean for each observation.

2.  For $k=1,2,\dots,p$:

    1.  Fit all $\binom{p}{k}$ models that contain exactly k predictors

    2.  Pick the best among these $\binom{p}{k}$ models, and call it
        $M_k$. Where *best* is the one with the smallest $SSR$

3.  Select a single best model from among $M_0,\dots, M_p$ using
    cross-validated prediction error.

Stepwise Selection
------------------

### Stepwise Selection

-   For computational reasons, best subset selection cannot be applied
    with very large p.

-   Best subset selection may also suffer from statistical problems when
    p is large

-   An enormous search space can lead to overfitting and high variance
    of the coefficient estimates.

-   For both of these reasons, stepwise methods, which explore a far
    more restricted set of models, are attractive alternatives to best
    subset selection.

### Stepwise Selection

1.  Forward Stepwise Selection

    -   Start with no predictors

    -   Test all models with 1 predictor. Choose the best model

    -   Add 1 predictor at a time, without taking away.

    -   Of the p+1 models, choose the one with smallest prediction error
        using cross validation

2.  Backward Stepwise Selection

    -   Same idea but start with a complete model and go backwards,
        taking one at a time.

### Forward Stepwise Selection

-   Computational advantage over best subset selection is clear.

-   It is not guaranteed to find the best possible model out of all
    $2^p$ models containing subsets of the p predictors.

-   Drawback: once a predictor enters, it cannot leave.

### Backward Stepwise Selection

-   Like forward stepwise selection, the backward selection approach
    searches through only $1 + p(p + 1)/2$ models

-   However, unlike forward stepwise selection, it begins with the model
    containing all p predictors, and then iteratively removes the least
    useful predictor, one-at-a-time.

-   Like forward stepwise selection, backward stepwise selection is not
    guaranteed to yield the best model containing a subset of the p
    predictors.

-   Backward selection requires that the number of observations
    (samples) $n$ is larger than the number of variables $p$ (so that
    the full model can be fit).

-   In contrast, forward stepwise can be used even when $n < p$, and so
    is the only viable subset method when p is very large.

Regularización
--------------

### Recap: OLS Mechanics

\<1\>\[label=motivacion\]

### Regularización: Motivación

-   Las técnicas econometricas estándar no están optimizadas para la
    predicción porque se enfocan en la insesgadez.

-   OLS por ejemplo es el mejor estimador lineal *insesgado*

-   OLS minimiza el error *"dentro de muestra"*, eligiendo $\beta$ de
    forma tal que

    $$\begin{aligned}
    min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 \end{aligned}$$

-   pero para predicción, no estamos interesados en hacer un buen
    trabajo dentro de muestra

-   Queremos hacer un buen trabajo, **fuera de muestra**

### OLS 1 Dimension

$$\begin{aligned}
 min_{\beta} E(\beta)=\sum_{i=1}^n (y_i-x_i \beta)^2 \end{aligned}$$

![image](figures/lasso0.pdf)

[App](https://cede.uniandes.edu.co/OLS/)

### OLS 2 Dimensiones

$$\begin{aligned}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i - x_{i1}\beta_1-x_{i2}\beta_2)^2 \end{aligned}$$

![image](figures/ols1.png)

Fuente: <https://allmodelsarewrong.github.io>

[App](https://cede.uniandes.edu.co/OLS/)

### Ridge

### Regularización

-   Asegurar cero sesgo dentro de muestra crea problemas fuera de
    muestra: trade-off Sesgo-Varianza

-   Las técnicas de machine learning fueron desarrolladas para hacer
    este trade-off de forma empírica.

-   Vamos a proponer modelos del estilo

    $$\begin{aligned}
    min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p R(\beta_j)\end{aligned}$$

-   donde $R$ es un regularizador que penaliza funciones que crean
    varianza

### Ridge

-   Para un $\lambda \geq 0$ dado, consideremos ahora el siguiente
    problema de optimización

    $$\begin{aligned}
    min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p (\beta_j)^2\end{aligned}$$

### Ridge: Intuición en 1 Dimension 

-   1 predictor estandarizado

-   El problema: $$\begin{aligned}
    min_{\beta} E(\beta) = \sum_{i=1}^n (y_i- x_{i}\beta )^2 + \lambda \beta^2\end{aligned}$$

-   La solución?

    [App](https://cede.uniandes.edu.co/OLS/)

### Ridge: Intuición en 1 Dimension 

#### Problema como optimización restringida

-   Existe un $c\geq 0$ tal que $\hat{\beta}(\lambda)$ es la solución a

    $$\begin{aligned}
         min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i}\beta)^2  \\ \nonumber
         & \text{sujeto a}   \\
         &  (\beta)^2  \leq c \nonumber
      \end{aligned}$$

### Ridge: Intuición en 2 Dimensiones 

-   Al problema en 2 dimensiones podemos escribirlo como

    $$\begin{aligned}
      min_{\beta} E(\beta) = \sum_{i=1}^n (y_i  - x_{i1}\beta_1  - x_{i2}\beta_2 + \lambda  \left(\beta_1^2 + \beta_2^2 \right)
      \end{aligned}$$

-   podemos escribirlo como un problema de optimización restringido
    $$\begin{aligned}
         min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \\ \nonumber
         & \text{sujeto a}   \\
         & \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c \nonumber
      \end{aligned}$$

### Ridge: Intuición en 2 Dimensiones 

$$\begin{aligned}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c 
  \end{aligned}$$

![image](figures/ridge1.png)

Fuente: <https://allmodelsarewrong.github.io>

### Ridge: Intuición en 2 Dimensiones 

$$\begin{aligned}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c 
  \end{aligned}$$

![image](figures/ridge2.png)

Fuente: <https://allmodelsarewrong.github.io>

### Ridge: Intuición en 2 Dimensiones 

$$\begin{aligned}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c 
  \end{aligned}$$

![image](figures/ridge3.png)

Fuente: <https://allmodelsarewrong.github.io>

### Ridge: Intuición en 2 Dimensiones 

$$\begin{aligned}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( (\beta_1)^2 + (\beta_2)^2 \right) \leq c 
  \end{aligned}$$

![image](figures/ridge4.png)

Fuente: <https://allmodelsarewrong.github.io>

### Términos generales

-   En regresión multiple ($X$ es una matriz $n\times k$)

-   Regresión: $y= X \beta   + u$

-   OLS

    $$\hat{\beta}_{ols}= (X'X)^{-1}X'y$$

-   Ridge $$\hat{\beta}_{ridge}= (X'X+ \lambda I)^{-1}X'y$$

### Ridge vs OLS

-   Ridge es sesgado $E(\hat{\beta}_{ridge}) \neq \beta$

-   Pero la varianza es menor que la de OLS

-   Para ciertos valores del parámetro
    $\lambda \Rightarrow MSE_{OLS}>MSE_{ridge}$

### Escala de las variables

### Escala de las variables

-   La escala de las variables importa en Ridge, mientras que en OLS no.

-   Tiene consecuencias

    -   En la solución ($\hat{\beta}$)

    -   En la predicción ($\hat{y}$)

### Escala de las variables

#### Ridge no es invariante a las escala

-   Supongamos $z=c*x$

-   Vamos a mostrar que $\hat{y}^z_{i}=\hat{y}^x_{i}$

-   Partamos del modelo $$\begin{aligned}
      y_{i}=\beta^z_{0}+\beta^z_{1}z_i+u
      \end{aligned}$$ $$\begin{aligned}
        \hat{\beta}^z_{1} = \frac{\sum(z_{i}-\bar{z})(y_{i}-\bar{y})}{\sum(z_{i}-\bar{z})^{2}}
      \end{aligned}$$

### Escala de las variables

#### Ridge no es invariante a las escala

-   Continuando $$\begin{aligned}
      \hat{\beta}^z_{1} = \frac{\sum(z_{i}-\bar{z})(y_{i}-\bar{y})}{\sum(z_{i}-\bar{z})^{2}}
      \end{aligned}$$

-   Pero $z=c*x$ $$\begin{aligned}
      \hat{\beta}^z_{1} &= \frac{\sum(cx_{i}-c\bar{x})(y_{i}-\bar{y})}{\sum(cx_{i}-\bar{cx})^{2}} \\
                        &= \frac{1}{c}\frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum(x_{i}-\bar{x})^{2}} \\
                        &= \frac{1}{c}\hat{\beta}^x_1
      \end{aligned}$$

-   En Ridge?

### Escala de las variables

#### Ridge no es invariante a las escala

-   Para un $\lambda \geq 0$ dado, el problema de optimización

    $$\begin{aligned}
    min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta^z_{0}-\beta^z_{1}z_i)^2 + \lambda (\beta^z_{1})^2\end{aligned}$$

-   Demo: [baticomputer](), math: Homework

### Escala de las variables

#### Ridge no es invariante a las escala

-   En la predicción

    $$\begin{aligned}
      \hat{\beta}^z_{1}z_i &=\hat{\beta}^z_{1}cx_i \end{aligned}$$
    $$\begin{aligned}
      =\frac{1}{c}\hat{\beta}^x_1cx_i 
      \end{aligned}$$ $$\begin{aligned}
      =\hat{\beta}^x_1 x_i
      \end{aligned}$$

### Escala de las variables

#### Ridge no es invariante a las escala

-   En términos generales, si $Z=cX$ $$\begin{aligned}
        \hat{\beta}^Z_{OLS}  &= (Z'Z)^{-1} Z' y \\
                             &= ((cX)'(cX))^{-1} (cX)' y \\
                             &= \frac{c}{c^2}(X'X)^{-1} X' y \\
                             &= \frac{1}{c}(X'X)^{-1} X' y  \\
                             &= \frac{1}{c} \hat{\beta}^X_{OLS}
      \end{aligned}$$

### Escala de las variables

#### Ridge no es invariante a las escala

-   Entonces $$\begin{aligned}
      \hat{\beta}^Z_{OLS} Z  &=  \frac{1}{c} \hat{\beta}^X_{OLS} cX \\
                             &= \hat{\beta}^X_{OLS} X
      \end{aligned}$$

-   Con Ridge esto no funciona $$\begin{aligned}
      \hat{\beta}^Z_{Ridge} Z &\neq  \hat{\beta}^X_{Ridge} X
      \end{aligned}$$

-   Es importante estandarizar las variables (la mayoría de los
    softwares lo hace automáticamente)

### 

![image](../banner/baticomputer_meme.jpg)\
photo from
<https://www.dailydot.com/parsec/batman-1966-labels-tumblr-twitter-vine/>

### Selección de $\lambda$

### Selección de $\lambda$ 

-   Asegurar cero sesgo dentro de muestra crea problemas fuera de
    muestra: trade-off Sesgo-Varianza

-   Ridge hace este trade-off de forma empírica. $$\begin{aligned}
                min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p R(\beta_j)
        \end{aligned}$$

-   $\lambda$ es el precio al que hacemos este trade off

-   Como elegimos $\lambda$?

### Selección de $\lambda$ 

-   $\lambda$ es un hiper-parámetro y lo elegimos usando validación
    cruzada

    -   Partimos la muestra de entrenamiento en K Partes:

        $MUESTRA= M_{fold\,1} \cup M_{fold\,2} \dots \cup M_{fold\,K}$

    -   Cada conjunto $M_{fold\,K}$ va a jugar el rol de una muestra de
        evaluación $M_{eval\,k}$.

    -   Entonces para cada muestra

        -   $M_{train-1}=M_{train} - M_{fold\,1}$

        -   $\vdots$

        -   $M_{train-k}=M_{train} - M_{fold\,k}$

### Selección de $\lambda$ 

-   Luego hacemos el siguiente loop

    -   Para $i=0,0.001,0.002,\dots,\lambda_{max}$ $\{$\
        - Para $k=1,\dots,K$ $\{$\
        $\,\,\,\,\,\,$- Ajustar el modelo $m_{i,k}$ con $\lambda_i$ en
        $M_{train-k}$\
        $\,\,\,\,\,\,$- Calcular y guardar el $MSE(m_{i,k})$ usando
        $M_{eval-k}$\
        $\}$ *\# fin para k*\
        - Calcular y guardar $MSE_i=\frac{1}{K} MSE(m_{i,k})$\
        $\}$ *\# fin para $\lambda$*

-   Encontramos el menor $MSE_i$ y usar ese $\lambda_i=\lambda^*$

### 

![image](../banner/baticomputer_meme.jpg)\
photo from
<https://www.dailydot.com/parsec/batman-1966-labels-tumblr-twitter-vine/>

### Ridge as Data Augmentation 

### Ridge as Data Augmentation (1)

-   Add $\lambda$ additional points

    $$\begin{aligned}
     \sum_{i=1}^n (y_i - x_{i}\beta)^2 + \lambda \beta^2 &= \\
     &=\sum_{i=1}^n (y_i - x_{i}\beta)^2 + \sum_{j=1}^\lambda (0- \beta)^2 \\
     &=\sum_{i=1}^{n+\lambda} (y_i - x_{i}\beta)^2 \end{aligned}$$

[RidgeDataAug](https://cede.uniandes.edu.co/OLS/)

### Ridge as Data Augmentation (2)

-   Add a single point

    $$\begin{aligned}
        \sum_{i=1}^n (y_i - x_{i}\beta)^2 + \lambda \beta^2 &= \\
        &=\sum_{i=1}^n (y_i - x_{i}\beta)^2 + (0-\sqrt{\lambda} \beta)^2\\
        &= \sum_{i=1}^{n+1} (y_i - x_{i}\beta)^2 
       \end{aligned}$$

[RidgeDataAug](https://cede.uniandes.edu.co/OLS/)

### More predictors than observations ($k>n$)

-   What happens when we have more predictors than observations ($k>n$)?

    -   OLS fails

    -   Ridge ?

### OLS when $k>n$

-   Rank? Max number of rows or columns that are linearly independent

    -   Implies $rank(X_{n\times k}) \leq min(k,n)$

-   MCO we need $rank(X_{n\times k})=k \implies k\leq n$

-   If $rank(X_{n\times k})=k$ then $rank(X'X)=k$

-   If $k>n$, then $rank(X'X)\leq n < k$ then $(X'X)$ cannot be inverted

-   Ridge works when $k \geq n$

### Ridge when $k>n$

$$\begin{aligned}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\sum^k_{j=1}x_{ij}'\beta_j)^2 + \lambda  (\sum^k_{j=1} \beta_j)^2\end{aligned}$$

-   Solution $\rightarrow$ data augmentation

-   Intuition: Ridge "adds" $k$ additional points.

-   Allows us to "deal" with $k\geq n$

Lasso
-----

### Lasso

-   Para un $\lambda \geq 0$ dado, consideremos el siguiente problema de
    optimización

    $$\begin{aligned}
    min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-\beta_0 - x_{i1}\beta_1 - \dots - x_{ip}\beta_p)^2 + \lambda \sum_{j=1}^p |\beta_j| \end{aligned}$$

    -   "LASSO's free lunch": selecciona automáticamente los predictores
        que van en el modelo ($\beta_j \neq 0$) y los que no
        ($\beta_j = 0$)

    -   Por qué? Los coeficientes que no van son soluciones de esquina

    -   $L(\beta)$ es no differentiable

### Lasso Intuición en 1 Dimension

$$\begin{aligned}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda|\beta| \end{aligned}$$

-   Un solo predictor, un solo coeficiente

-   Si $\lambda=0$ $$\begin{aligned}
      min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 
      \end{aligned}$$

-   y la solución es $$\begin{aligned}
        \hat{\beta}_{OLS}
      \end{aligned}$$

### Intuición en 1 Dimension

$$\begin{aligned}
 min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda|\beta| \end{aligned}$$

### Intuición en 1 Dimensión

####  $\hat{\beta}>0$

$$\begin{aligned}
 min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda \beta \end{aligned}$$

![image](figures/lasso1.pdf)

### Intuición en 1 Dimensión

####  $\hat{\beta}>0$

$$\begin{aligned}
 min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda \beta \end{aligned}$$

![image](figures/lasso2.pdf)

### Intuición en 1 Dimensión

####  $\hat{\beta}>0$

$$\begin{aligned}
 min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda \beta \end{aligned}$$

![image](figures/lasso3.pdf)

### Intuición en 1 Dimensión

####  $\hat{\beta}>0$

$$\begin{aligned}
 min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda \beta \end{aligned}$$

![image](figures/lasso4.pdf)

### Intuición en 1 Dimensión

####  $\hat{\beta}>0$

$$\begin{aligned}
 min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda \beta \end{aligned}$$

![image](figures/lasso5.pdf)

### Intuición en 1 Dimensión

####  $\hat{\beta}>0$

$$\begin{aligned}
 min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda \beta \end{aligned}$$

![image](figures/lasso6.pdf)

### Intuición en 1 Dimensión

####  $\hat{\beta}>0$

$$\begin{aligned}
 min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda \beta \end{aligned}$$

![image](figures/lasso_final.pdf)

### Ilustración en R

![image](../banner/baticomputer_meme.jpg)\
photo from
<https://www.dailydot.com/parsec/batman-1966-labels-tumblr-twitter-vine/>

### Intuición en 1 Dimension

#### Solución analitica

$$\begin{aligned}
min_{\beta} E(\beta) = \sum_{i=1}^n (y_i-x_i \beta)^2 + \lambda|\beta| \end{aligned}$$

-   la solución analítica es

$$\begin{aligned}
\hat{\beta}_{lasso}=\begin{cases}
0 & \text{\text{si} }\ensuremath{\lambda\geq \lambda^*}\\
\hat{\beta}_{OLS}-\frac{\lambda}{2} & \text{\text{si} }\ensuremath{\lambda<\lambda^*}
\end{cases}\end{aligned}$$

### Intuición en 2 Dimensiones (Lasso)

$$\begin{aligned}
     min_{\beta} E(\beta) &= \sum_{i=1}^n (y_i - x_{i1}\beta_1 - x_{i1}\beta_2)^2  \text{ s.a }   \left( |\beta_1| + |\beta_2| \right) \leq c 
  \end{aligned}$$

![image](figures/lasso_ridge2.png)

Fuente: <https://allmodelsarewrong.github.io>

### Example

![image](../banner/baticomputer_meme.jpg)\
photo from
<https://www.dailydot.com/parsec/batman-1966-labels-tumblr-twitter-vine/>

### Resumen

-   Ridge y Lasso son sesgados, pero las disminuciones en varianza
    pueden compensar estoy y llevar a un MSE menor

-   Lasso encoje a cero, Ridge no tanto

-   Importante para aplicación:

    -   Estandarizar los datos

    -   Como elegimos $\lambda$? $\rightarrow$ Validación cruzada

Familia de regresiones penalizadas
----------------------------------

### Family of penalized regressions

$$\begin{aligned}
min_{\beta} R(\beta) = \sum_{i=1}^n (y_i-x_i'\beta)^2 + \lambda \sum_{s=2}^p |\beta_s|^p\end{aligned}$$

![image](figures/penalties.png)

$k>n$
-----

### More predictors than observations ($k>n$)

-   Objective 1: Accuracy

    -   Minimize prediction error (in one step) $\rightarrow$ Ridge,
        Lasso

-   Objective 2: Dimensionality

    -   Reduce the predictor space $\rightarrow$ Lasso's free lunch

```{=html}
<!-- -->
```
-   What happens when we have more predictors than observations ($k>n$)?

    -   OLS fails

    -   Ridge augments data

    -   and Lasso?

### Lasso when $k>n$

-   Lasso works fine in this case

-   However, there are some issues to keep in mind

    -   When $k>n$ chooses at most $n$ variables

    -   When we have a group of highly correlated variables,

        -   Lasso chooses only one. Makes it unstable for prediction.
            (Doesn't happen to Ridge)

        -   Ridge shrinks the coefficients of correlated variables
            toward each other. This makes Ridge "work" better than
            Lasso. "Work" in terms of prediction error

Elastic Net
-----------

### Elastic net

$$\begin{aligned}
min_{\beta} EN(\beta) &= \sum_{i=1}^n (y_i-\beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2  + \lambda\left(\alpha \sum_{j=1}^p |\beta_j| + \frac{(1-\alpha)}{2} \sum_{j=1}^p (\beta_j)^2\right)\end{aligned}$$

-   Si $\alpha=1$ Lasso

-   Si $\alpha=0$ Ridge

### Elastic Net

-   Elastic net: happy medium.

    -   Good job at prediction and selecting variables

$$\begin{aligned}
min_{\beta} EN(\beta) &= \sum_{i=1}^n (y_i-\beta_0 - \sum_{j=1}^p x_{ij}\beta_j)^2  + \lambda\left(\alpha \sum_{j=1}^p |\beta_j| + \frac{(1-\alpha)}{2} \sum_{j=1}^p (\beta_j)^2\right)\end{aligned}$$

-   Mixes Ridge and Lasso

-   Lasso selects predictors

-   Strict convexity part of the penalty (ridge) solves the grouping
    instability problem

-   How to choose $(\lambda,\alpha)$? $\rightarrow$ Bidimensional
    Crossvalidation

-   Recomended lecture: Zou, H. & Hastie, T. (2005)

-   H.W.: $\beta_{OLS}>0$ one predictor standarized $$\begin{aligned}
    \hat{\beta}_{EN}= \frac{\left(\hat{\beta}_{OLS}-\frac{\lambda_1}{2}\right)_{+}}{1+\lambda_2}\end{aligned}$$

### Example

![image](../banner/baticomputer_meme.jpg)\
photo from
<https://www.dailydot.com/parsec/batman-1966-labels-tumblr-twitter-vine/>
