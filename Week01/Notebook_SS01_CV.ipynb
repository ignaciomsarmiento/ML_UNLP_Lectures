{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5653c385",
   "metadata": {},
   "source": [
    "<div >\n",
    "    <img src = \"../banner/banner_ML_UNLP_1900_200.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24272767",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ignaciomsarmiento/ML_UNLP_Lectures/blob/main/Week01/Notebook_SS01_CV.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e57de4",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44efaa5f-efe9-424e-bf07-c46550bc67fb",
   "metadata": {},
   "source": [
    "The concept behind resampling techniques for evaluating model performance is straightforward: a portion of the data is used to train the model, while the remaining data is used to assess the model's accuracy. \n",
    "\n",
    "This process is repeated several times with different subsets of the data, and the results are averaged and summarized. The primary differences between resampling techniques lie in the method by which the subsets of data are selected. \n",
    "\n",
    "In the following sections, we will discuss the main types of resampling techniques.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a3dcd",
   "metadata": {},
   "source": [
    "# Predicting Wages\n",
    "\n",
    "Our objective today is to construct a model of individual wages\n",
    "\n",
    "$$\n",
    "w = f(X) + u \n",
    "$$\n",
    "\n",
    "where w is the  wage, and X is a matrix that includes potential explanatory variables/predictors. In this problem set, we will focus on a linear model of the form\n",
    "\n",
    "\\begin{align}\n",
    " ln(w) & = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p  + u \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ba2f4",
   "metadata": {},
   "source": [
    "were $ln(w)$ is the logarithm of the wage.\n",
    "\n",
    "To illustrate I'm going to use a sample of the NLSY97. The NLSY97 is  a nationally representative sample of 8,984 men and women born during the years 1980 through 1984 and living in the United States at the time of the initial survey in 1997.  Participants were ages 12 to 16 as of December 31, 1996.  Interviews were conducted annually from 1997 to 2011 and biennially since then.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab5a6",
   "metadata": {},
   "source": [
    "Let's load the modules and the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "516fd2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2800e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlsy=pd.read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/nlsy97.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e2ca2",
   "metadata": {},
   "source": [
    "what are the predictors that I have available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7208cb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lnw_2016</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>other</th>\n",
       "      <th>exp</th>\n",
       "      <th>afqt</th>\n",
       "      <th>mom_educ</th>\n",
       "      <th>dad_educ</th>\n",
       "      <th>yhea_100_1997</th>\n",
       "      <th>...</th>\n",
       "      <th>_XPexp_13</th>\n",
       "      <th>_XPexp_14</th>\n",
       "      <th>_XPexp_16</th>\n",
       "      <th>_XPexp_17</th>\n",
       "      <th>_XPexp_18</th>\n",
       "      <th>_XPexp_19</th>\n",
       "      <th>_XPexp_20</th>\n",
       "      <th>_XPexp_21</th>\n",
       "      <th>_XPexp_22</th>\n",
       "      <th>_XPexp_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.076898</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>7.0724</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.294138</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4.7481</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.830896</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1.1987</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.306459</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>8.9321</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.991465</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2.2618</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 994 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lnw_2016  educ  black  hispanic  other  exp    afqt  mom_educ  dad_educ  \\\n",
       "0  4.076898    16      0         0      0   11  7.0724        12        12   \n",
       "1  3.294138     9      0         0      0   19  4.7481         9        10   \n",
       "2  2.830896     9      0         1      0   22  1.1987        12         9   \n",
       "3  4.306459    16      0         0      0   13  8.9321        16        18   \n",
       "4  5.991465    16      0         1      0   15  2.2618        16        16   \n",
       "\n",
       "   yhea_100_1997  ...  _XPexp_13  _XPexp_14  _XPexp_16  _XPexp_17  _XPexp_18  \\\n",
       "0              3  ...          0          0          0          0          0   \n",
       "1              2  ...          0          0          0          0          0   \n",
       "2              3  ...          0          0          0          0          0   \n",
       "3              2  ...          1          0          0          0          0   \n",
       "4              1  ...          0          0          0          0          0   \n",
       "\n",
       "   _XPexp_19  _XPexp_20  _XPexp_21  _XPexp_22  _XPexp_23  \n",
       "0          0          0          0          0          0  \n",
       "1          1          0          0          0          0  \n",
       "2          0          0          0          1          0  \n",
       "3          0          0          0          0          0  \n",
       "4          0          0          0          0          0  \n",
       "\n",
       "[5 rows x 994 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlsy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d1f92",
   "metadata": {},
   "source": [
    "Let's keep a couple of these predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800e2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nlsy[[ \"educ\", \"black\", \"hispanic\", \"other\", \"exp\", \"afqt\", \"mom_educ\", \"dad_educ\"]]\n",
    "\n",
    "y=nlsy[[\"lnw_2016\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68c5fd",
   "metadata": {},
   "source": [
    "The descriptive statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab208b30-4a14-475f-9ed4-026b2644b121",
   "metadata": {
    "results": "asis",
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lnw_2016</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>other</th>\n",
       "      <th>exp</th>\n",
       "      <th>afqt</th>\n",
       "      <th>mom_educ</th>\n",
       "      <th>dad_educ</th>\n",
       "      <th>yhea_100_1997</th>\n",
       "      <th>...</th>\n",
       "      <th>_XPexp_13</th>\n",
       "      <th>_XPexp_14</th>\n",
       "      <th>_XPexp_16</th>\n",
       "      <th>_XPexp_17</th>\n",
       "      <th>_XPexp_18</th>\n",
       "      <th>_XPexp_19</th>\n",
       "      <th>_XPexp_20</th>\n",
       "      <th>_XPexp_21</th>\n",
       "      <th>_XPexp_22</th>\n",
       "      <th>_XPexp_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.00000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "      <td>1266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.109862</td>\n",
       "      <td>14.314376</td>\n",
       "      <td>0.181675</td>\n",
       "      <td>0.116904</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>14.582148</td>\n",
       "      <td>5.325346</td>\n",
       "      <td>13.302528</td>\n",
       "      <td>13.253555</td>\n",
       "      <td>1.795419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101106</td>\n",
       "      <td>0.098736</td>\n",
       "      <td>0.095577</td>\n",
       "      <td>0.095577</td>\n",
       "      <td>0.096367</td>\n",
       "      <td>0.06951</td>\n",
       "      <td>0.021327</td>\n",
       "      <td>0.017378</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.004739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.869230</td>\n",
       "      <td>2.900304</td>\n",
       "      <td>0.385728</td>\n",
       "      <td>0.321432</td>\n",
       "      <td>0.084048</td>\n",
       "      <td>3.237619</td>\n",
       "      <td>2.914915</td>\n",
       "      <td>2.401108</td>\n",
       "      <td>2.908025</td>\n",
       "      <td>0.827403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301588</td>\n",
       "      <td>0.298425</td>\n",
       "      <td>0.294126</td>\n",
       "      <td>0.294126</td>\n",
       "      <td>0.295210</td>\n",
       "      <td>0.25442</td>\n",
       "      <td>0.144529</td>\n",
       "      <td>0.130725</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.068707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.955389</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.641836</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.864825</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.077225</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.339800</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.524779</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>7.958175</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.393263</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 994 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lnw_2016         educ        black     hispanic        other  \\\n",
       "count  1266.000000  1266.000000  1266.000000  1266.000000  1266.000000   \n",
       "mean      3.109862    14.314376     0.181675     0.116904     0.007109   \n",
       "std       0.869230     2.900304     0.385728     0.321432     0.084048   \n",
       "min      -1.955389     8.000000     0.000000     0.000000     0.000000   \n",
       "25%       2.641836    12.000000     0.000000     0.000000     0.000000   \n",
       "50%       3.077225    14.000000     0.000000     0.000000     0.000000   \n",
       "75%       3.524779    16.000000     0.000000     0.000000     0.000000   \n",
       "max       7.393263    20.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               exp         afqt     mom_educ     dad_educ  yhea_100_1997  ...  \\\n",
       "count  1266.000000  1266.000000  1266.000000  1266.000000    1266.000000  ...   \n",
       "mean     14.582148     5.325346    13.302528    13.253555       1.795419  ...   \n",
       "std       3.237619     2.914915     2.401108     2.908025       0.827403  ...   \n",
       "min       7.000000     0.000000     4.000000     3.000000       1.000000  ...   \n",
       "25%      12.000000     2.864825    12.000000    12.000000       1.000000  ...   \n",
       "50%      15.000000     5.339800    12.000000    12.000000       2.000000  ...   \n",
       "75%      17.000000     7.958175    15.000000    16.000000       2.000000  ...   \n",
       "max      23.000000    10.000000    20.000000    20.000000       5.000000  ...   \n",
       "\n",
       "         _XPexp_13    _XPexp_14    _XPexp_16    _XPexp_17    _XPexp_18  \\\n",
       "count  1266.000000  1266.000000  1266.000000  1266.000000  1266.000000   \n",
       "mean      0.101106     0.098736     0.095577     0.095577     0.096367   \n",
       "std       0.301588     0.298425     0.294126     0.294126     0.295210   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "        _XPexp_19    _XPexp_20    _XPexp_21    _XPexp_22    _XPexp_23  \n",
       "count  1266.00000  1266.000000  1266.000000  1266.000000  1266.000000  \n",
       "mean      0.06951     0.021327     0.017378     0.003949     0.004739  \n",
       "std       0.25442     0.144529     0.130725     0.062745     0.068707  \n",
       "min       0.00000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.00000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.00000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.00000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.00000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 994 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlsy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2532d7",
   "metadata": {},
   "source": [
    "# Validation Set  Approach\n",
    "\n",
    "The first method to evaluate out-of-sample performance is the validation set approach. In this approach, a fixed portion of the data is designated as the validation set, and the model is trained on the remaining data. The model's performance is then evaluated on the validation set. These partitions are usually called:\n",
    "\n",
    "   - Training sample: to build/estimate/train the model\n",
    "   - Testing (validation, hold-out) sample:  to evaluate its performance \n",
    "\n",
    "Partitions can be of any size. Usually, 70%-30% or 80%-20% are used. Graphically, a 70%-30% partition looks like:     \n",
    "    \n",
    "<div>\n",
    "<img src=\"figs_notebook/30-70.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88212caa",
   "metadata": {},
   "source": [
    "Let's implement this in `R`.\n",
    "\n",
    "We begin by generating a sample index that will indicate with `TRUE` those observations randomly assigned to the training data set with 70% probability, and with `FALSE` those observations randomly assigned to the testing data set with 30% chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acae014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X,\n",
    "                                        y,\n",
    "                                        test_size=0.3,\n",
    "                                        train_size=0.7,\n",
    "                                        random_state = 123\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b63d02",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "we can check that the partition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbea998",
   "metadata": {},
   "source": [
    "With the above index, we generate the partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de9d0077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>other</th>\n",
       "      <th>exp</th>\n",
       "      <th>afqt</th>\n",
       "      <th>mom_educ</th>\n",
       "      <th>dad_educ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>9.1840</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.5646</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.1047</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>2.6821</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4.2699</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      educ  black  hispanic  other  exp    afqt  mom_educ  dad_educ\n",
       "644     16      0         0      0   12  9.1840        14        14\n",
       "1051    11      0         1      0   17  0.5646        11        15\n",
       "208     14      0         0      0   15  1.1047        12        12\n",
       "1130    13      0         1      0   17  2.6821         7        11\n",
       "114     17      0         0      0   10  4.2699        16        14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1318f0-248e-41cf-b5c6-3a9fec9f9d99",
   "metadata": {},
   "source": [
    "## Predicting wages\n",
    "\n",
    "With these partitions in place, we can start building our predictive models. We begin by using a simple model with no covariates, just a constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a201763-d401-4f93-866a-00d1a6dc87c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.07664019])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = np.ones((len(y_train), 1))\n",
    "model1=  LinearRegression().fit(X0,y_train)\n",
    "model1.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980df5e-4174-45bb-9b0f-13b558828a0b",
   "metadata": {},
   "source": [
    "In this case, the prediction for the log wage is the average train sample average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ad357-a9c5-4be6-9a38-e379229878b3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y}=\\hat{\\beta_1}=\\frac{\\sum y_i}{n}=m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f257e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lnw_2016    3.07664\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e78c6-94e3-4646-b96b-269fcffcee7b",
   "metadata": {},
   "source": [
    "Since we are concerned with predicting well out-of -sample, we need to evaluate our model in the testing data. For that, we use the coefficient estimated in the training data and use it as a predictor in the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbea972-26d5-48a1-a322-9c0a942703a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on new data\n",
    "X0_test = np.ones((len(y_test), 1))\n",
    "y_hat_model1 = model1.predict(X0_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77839d42-8882-4761-8db4-df6bc614f3ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Then we can calculate the out-of-sample performance using the MSE:\n",
    "\n",
    "$$\n",
    "test\\,MSE=E((y-\\hat{y})^2)\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "158d9d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.8006973615519469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse1 = mean_squared_error(y_test, y_hat_model1)\n",
    "\n",
    "print(f'Mean Squared Error: {mse1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd03f2",
   "metadata": {},
   "source": [
    "This is quite a naive model that uses the sample average as a prediction. \n",
    "\n",
    "Let's see if we can improve (reduce the prediction error) this model.\n",
    "\n",
    "To improve our prediction, we can start adding explanatory variables. Let's begin by adding only one variable,  `education (educ)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbe39dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07838887]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model2=  LinearRegression().fit(X_train[['educ']],y_train)\n",
    "model2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f0a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on new data\n",
    "y_hat_model2 = model2.predict(X_test[['educ']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d11ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "and evaluate the  out-of-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb9cac15-8f34-4962-9bf1-c3a680bd6f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.7561918386513357\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean Squared Error\n",
    "mse2 = mean_squared_error(y_test, y_hat_model2)\n",
    "\n",
    "print(f'Mean Squared Error: {mse2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311451b-4af0-4638-a834-fa921cee1c71",
   "metadata": {},
   "source": [
    "There's a clear diminution in MSE. Let's add complexity by adding more variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c152233-8bda-4e61-9853-160fb7790093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06818859, 0.02310836, 0.05186946, 0.01073129, 0.00121854]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3=  LinearRegression().fit(X_train[['educ','exp','afqt','mom_educ','dad_educ']],y_train)\n",
    "model3.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ccf7b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e06f8287-7756-4d88-99a2-2eb1285007fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.7435323359911381\n"
     ]
    }
   ],
   "source": [
    "#prediction on new data\n",
    "y_hat_model3 = model3.predict(X_test[['educ','exp','afqt','mom_educ','dad_educ']])\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse3 = mean_squared_error(y_test, y_hat_model3)\n",
    "\n",
    "print(f'Mean Squared Error: {mse3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129749c0-3299-475d-9448-8b8d194c253e",
   "metadata": {},
   "source": [
    "In this case, the MSE keeps improving. Is there a limit to this improvement? Can we keep adding features and complexity? What about an extremely complex model that includes polynomials and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5fc101d-6211-40d9-8dc2-4e2f8580d568",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "?PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d467e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "\n",
    "\n",
    "model4 =  LinearRegression().fit(X_train_poly,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f22665",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "327d5853-5364-4865-8854-3066623e0517",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.9110896508614051\n"
     ]
    }
   ],
   "source": [
    "#prediction on new data\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "y_hat_model4 = model4.predict(X_test_poly)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse4 = mean_squared_error(y_test, y_hat_model4)\n",
    "\n",
    "print(f'Mean Squared Error: {mse4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0149927-e250-4ba5-94b0-0a76bf6428a9",
   "metadata": {},
   "source": [
    "It is clear that as complexity increases, performance improves until a point where too much complexity results in inferior performance. \n",
    "\n",
    "This is an illustration of the Bias-Variance-Trade-Off.\n",
    "\n",
    "Although the validation set approach is quite nice, there are at least two problems with it\n",
    "  \n",
    "  1. The first one is that given an original data set if part of it is left aside to test the model, fewer data is left for estimation (leading to less efficiency).\n",
    "  2. A second problem is deciding which data will be used to train the model and which one to test it:\n",
    "  \n",
    "  <div>\n",
    "<img src=\"figs_notebook/fig52.png\" width=\"800\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1586176",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Leave-One-Out Cross-Validation (LOOCV) \n",
    "\n",
    "This method is similar to the Validation Set Approach, but it tries to address the latter's disadvantages. Leave-One-Out Cross-Validation (LOOCV) is a resampling technique for evaluating model performance. Each sample in the data is used once as the validation set, and the model is trained on the remaining samples. \n",
    "\n",
    "Graphically the LOOCV looks like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d07a19",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"figs_notebook/1.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"figs_notebook/2.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"figs_notebook/3.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "<div>\n",
    "<img src=\"figs_notebook/20.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "LOOCV is computationally expensive, as a separate model has to be fit `n` times, where `n` is the number of samples in the data. However, LOOCV is more thorough in its model evaluation, as each sample is used as the validation set exactly once, giving a more comprehensive assessment of the model's performance.\n",
    "\n",
    "The LOOCV estimate for the test MSE is\n",
    "\n",
    "\\begin{align}\n",
    "LOOCV(n) &= \\frac{1}{n}\\sum MSE_{-i} \\\\ \n",
    "      &= \\frac{1}{n}\\sum(y_i -\\hat{y}_{-i})^2\n",
    "\\end{align}\n",
    "\n",
    "where $-i$ indicates that the model to obtain the prediction was trained in all observations except $i$.\n",
    "\n",
    "LOOCV is particularly useful in cases where the number of samples in the data is small, and the risk of overfitting is high.\n",
    "\n",
    "\n",
    "LOOCV is a special case of k-fold cross-validation, where k is equal to the number of samples in the data. Given that it's a particular case of k-fold cross-validation, we will implement this instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455606ee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "\n",
    "K-Fold Cross-Validation  is a widely used resampling technique for evaluating model performance. It involves dividing the data into k equally sized folds, where k is a user-defined constant. The model is then fit k times, with each fold used once as the validation set and the remaining k-1 folds used as the training set. This process results in k estimates of the model's performance, which can then be averaged to obtain an overall estimate. Graphically it looks like this:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"figs_notebook/fold.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "K-Fold Cross-Validation is a trade-off between the computational efficiency of the validation set approach and the thoroughness of LOOCV. On the one hand, K-Fold Cross-Validation is more computationally efficient than LOOCV, as the model is fit k times instead of n times, where n is the number of samples in the data. On the other hand, K-Fold Cross-Validation is less thorough than LOOCV, as each sample is used in the validation set k-1/k of the time, giving a less comprehensive assessment of the model's performance.However, K-Fold Cross-Validation is widely used in practice. It provides a good balance between computational efficiency and thoroughness while allowing the user to control the number of times the model fits.\n",
    "\n",
    "K-Fold Cross-Validation provides a more robust evaluation of the model's performance than the validation set approach.  In the validation set approach, a fixed portion of the data is used as the validation set, which can result in a suboptimal estimation of the model's performance if the validation set is not representative of the data. In contrast, K-Fold Cross-Validation ensures that each sample is used in the validation set exactly once, providing a more comprehensive assessment of the model's performance.\n",
    "\n",
    "To sum up, to implement K-Fold Cross-Validation, we need to:\n",
    "\n",
    "- Split the data into K parts $(n=\\sum_{j=1}^k n_j)$\n",
    "\n",
    "- Fit the model leaving out one of the folds $\\rightarrow$ $\\hat{y}_{-k}$\n",
    "  \n",
    "- Cycle through all k folds\n",
    " \n",
    "-  The CV(k) estimate for the test MSE is\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "CV_{(k)} &= \\frac{1}{k}\\sum_{j=1}^k MSE_j \\\\\n",
    "         &= \\frac{1}{k}\\sum_{j=1}^k (y_j^k-\\hat{y}_{-k})^{2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"figs_notebook/fig54.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53a0bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4288a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d2d3e",
   "metadata": {},
   "source": [
    "\n",
    "## Calculating the MSE\n",
    "\n",
    "\n",
    "Finally, we need to calculate the  CV(k) estimate for the test MSE. We know that it takes the form:\n",
    "\n",
    "\\begin{align}\n",
    "CV_{(k)} &= \\frac{1}{k}\\sum_{j=1}^k MSE_j \\\\\n",
    "         &= \\frac{1}{k}\\sum_{j=1}^k (y_j^k-\\hat{y}_{-k})^{2}\n",
    "\\end{align}\n",
    "\n",
    "To implment this formula we first need to calculate the MSE for each fold using `lapply`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48128eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE por iteraciÃ³n: [0.71481139 0.60598029 0.68506495 0.78234256 0.66691443]\n",
      "Media del MSE: 0.6910227247613737\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Resultados\n",
    "mse_scores = -scores  # Convertir a positivo\n",
    "print(f'MSE por iteraciÃ³n: {mse_scores}')\n",
    "print(f'Media del MSE: {mse_scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d974f661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.71481139, -0.60598029, -0.68506495, -0.78234256, -0.66691443])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89befb",
   "metadata": {},
   "source": [
    "\n",
    "# If we have enough data\n",
    "\n",
    "\n",
    "In some cases, where there's enough data, researchers may use both K-Fold Cross-Validation and the validation set approach in combination to evaluate the performance of a machine learning model. This can be useful when a researcher wants to obtain a more robust evaluation of the model's performance while maintaining computational efficiency.\n",
    "\n",
    "The following figure shows the strategy followed by Kleinberg et al. (2017) in their paper \"Human decisions and machine predictions\":\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"figs_notebook/human_decisions.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "This strategy prevents the machine learning algorithm from appearing to do well simply because it is being evaluated on data it has already seen. Moreover, they add an extra layer of protection to ensure that the results are not an artifact of unhelpful \"human data mining,\" adding a \"pure hold-out.\"\n",
    "\n",
    "By combining K-Fold Cross-Validation and the validation set approach, researchers can obtain a more comprehensive evaluation of the model's performance while maintaining computational efficiency. The specific combination of K-Fold Cross-Validation and the validation set approach will depend on the researcher's goals and the particular constraints of the study. When choosing a resampling technique, it is essential to carefully consider the trade-offs between computational efficiency and thoroughness.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
